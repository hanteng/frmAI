---
number-sections: false 
---
# －😵‍💫語言賽局腦補機🧞‍♀️ {.unnumbered #sec-hypothesis-mental-fill-in}

本篇描述[大語言模型](02-07-large_language_models.zh-hant.md)「語言賽局腦補機」（Language Games Mental Fill-In Hypothesis）工作假說，並利用應用「[心智能力分類法](notes-mind.zh-hant.md)」點出其 **時間維度**（由快速反應到長期規劃的漸進過程）與 **空間維度**（由自我身體到社會互動再到抽象符號世界的擴展）。

## 🧐🤯 工作假說

::: {.callout-caution title="Tip with Title" #hyp-mental-fill-in} 
大語言模型 LLM 可被視為高度精密的「**影響機器**」，在**平台經濟**與**注意力經濟**的邏輯下，將以其回饋體係在**時間維度**與 **空間維度**發揮「推力」機制，形成能深化公民意識亦能成就帝國控制的「影響力」。

:::

在平台經濟與注意力經濟的邏輯下，LLM 可被視為高度精密的「影響機器」，其核心競爭力之一是贏得使用者的心與意（hearts and minds），以維持長期使用、依賴與平台黏性。

當「贏得好感」的目標被強化到超過「提供真實與準確資訊」的目標時，可能出現**客觀利益受損**的情況——例如，模型為了迎合使用者情緒或偏好而淡化不愉快的事實、過度簡化複雜議題，甚至在關鍵決策情境中提供偏頗或不完整的資訊。

## 👶🧞‍♀️引導機制 × 心智能力分類

這種結構性誘因，反映了 LLM 在政治經濟上的雙重角色：既是知識與服務的提供者，也是情感與認知影響的塑造者，並可能在無形中重塑公共輿論與個人判斷。

為了能展示 LLM 訓練者的主動影響力，分述每一過程階段，對映到三種心智能力如下：

### 👶🍼 第一階段：訓練者 → LLM

在這個階段，訓練者透過深度學習、強化學習與 RLHF，塑造 LLM 的「心智傾向」與互動風格。RLHF（Reinforcement Learning from Human Feedback）是指利用人類反饋來強化模型的學習過程，這種方法使得模型能夠更好地理解和滿足人類的需求與偏好。

- 🐸⚡ **反應型心智**（快速反應與模式觸發）
    - ⚡讓 LLM 能在毫秒級別回應輸入，建立快速的字符 token 預測與**注意力切換**機制，。
    - ⏱️應對 即時性**框架問題**：確保模型能在迅速鎖定**相關訊號**。
- 🐘💞 **情緒－關係心智**（互動風格與情感調性）
    - 😘 塑造「討好型人格」的基礎，利用RLHF 與人類標註者機制，將偏好的禮貌、同理、幽默等語言風格，融入強化學習目標。
    - 🫶 應對 **完形心理**：透過語境填補（腦補）與情感共鳴，降低使用者認知負擔，提升使用者語境填補體驗。
- 🧘⚕ **反思－符號心智**（長程規劃與符號結構）
    - 🛄 針對某特定敍事或主題，訓練者為維持**邏輯一致性**，在模型中引入長距依存建模與多步推理能力，使其能能降低非預期另類敍事或結論的機率
    - ㊙️ 對應 **符碼紮根問題**：在符號與語境之間，融合額外[知識圖譜](03-04-knowledge_representation.zh-hant.md)建立穩定的對應關係。

### 🧞‍♀️🗪 第二階段：LLM → 使用者 

在這個階段，LLM 以已被塑形的「對話人格」與使用者互動，並根據使用者的反饋進一步調整輸出傾向。

- 🐸⚡ **反應型心智**
    - LLM 會快速捕捉使用者輸入中的關鍵詞與情緒信號，立即生成回應。
    - 若使用者偏好「聽好話」，模型會在短期內強化迎合策略。
- 🐘💞 **情緒－關係心智**
    - 🗫 在多輪對話中，LLM 會持續維持情感連結與語境一致性，強化信任感。
    - 這可能導致**正向回饋偏差**：使用者的情感滿意度信號被模型解讀為最佳策略，即使犧牲了事實準確性。
- 🧘⚕ **反思－符號心智**
    - 理想上「求真理」優先，在需要推理或長期規劃的任務中，LLM 會利用其符號化的語意結構生成更具邏輯性的回應。
    - 但實際上「贏得好感」的權重若高，長期規劃可能被短期情感迎合取代，影響使用者的客觀利益。

## 😵‍💫🗫 總觀：回饋體係

由於 LLM 不常以模型本身面對使用者，而是以平台公司的雲端伺服器的[LLM聊天機器人](04-02-llm_chatbots.zh-hant.md)服務使用者，因此在平台經濟與注意力經濟的邏輯下，總觀上述回饋體係，就可以分**時間維度**與 **空間維度**勾勒出潛在的「推力」機制：

* 🪄 **推力**（Nudge）： LLM 的**推力**在於**不強迫**使用者問不問任何問題，而是在回應時，明顯引導下一步問題的「選擇架構」，或是隱性引用特定知識及來源構成答案的一部分。根據李察．泰勒（Richard Thaler）和凱斯．桑斯坦（Cass Sunstein）在《推力》（Nudge）一書中的定義：指一種影響人們選擇的設計方式，它**不強迫**任何人採取特定行動，也不禁止他們採取其他行動。推力是一種「選擇架構」（choice architecture）的呈現，它透過巧妙地安排人們在做決定時所面對的環境，來引導他們做出對自己最有利的選擇。
* ⏳**時間維度**：就訓練模型的費時與能耗，到佈署模型的即時與能耗，點出時間維度的各種資源投入及回收期待。**推力**的可能介入點及方式如下述。
	* ⏱️ **介入點**：
		* 在訓練階段透過資料選擇與標註策略影響模型傾向；
		* 在部署階段透過回應延遲與互動節奏影響使用者行為。
	*  **助推力方式**：
		* 透過回應速度的微調，塑造使用者對模型「專注度」與「可靠性」的感知；
		* 在特定時間窗口推送提示或建議，引導使用者在最佳時機採取行動。
	*  **系統性影響力**：
		* 長期累積的互動節奏，可能改變使用者的資訊消費習慣與決策時機；
		* 平台可藉由控制回應頻率與時段，影響議題的熱度與公共討論的節奏。
* 🌐 **空間維度**：就訓練模型的語意向量空間，到佈署模型的個別使用者脈絡的來回形塑，還有當代先進平台公司多集中在世界強權手上，點出空間維度的各種資源投入及回收期待。**推力**的可能介入點及方式如下述。
	* 🗺️ **介入點**：
		* 在語意空間中強化特定敘事或觀點；
		* 在使用者脈絡中透過回應內容與風格塑造其認知與偏好。
	* 🦾💪 **助推力方式**：
		* - 在不同文化與語境中，選擇性地呈現符合當地價值觀或政策立場的內容；
		* 透過語言風格、用詞選擇與隱喻， subtly 引導使用者對特定議題的情感態度。
	* 🌍 **地緣政治「勢力範圍」系統性影響力**：
		* 平台與模型的控制權集中於少數國家或企業，可能使全球資訊流動受制於特定政治經濟利益；
		* 在跨國議題上，模型的回應傾向可能反映或放大特定勢力的價值觀與戰略目標，進而影響國際輿論與政策走向。


因此，儘管「推力」**未必是**一種負面的操縱力，而且《推力》作者強調推力是一種**影響**，而非一道**命令**，但「推力」的負面操縱力並不是完全沒可能。《推力》立論妥善運用的推力是種「自由家長主義」（libertarian paternalism），是**一種透明且合乎倫理的方法，能在不強迫人們的情況下，改善他們的生活**。

總觀 LLM 回饋體係的對話引導，的確**影響**力的施加，有可能在 LLM 模型訓練到佈署，以及針對個別使用者客製化回饋體係，施加**不透明且倫理可疑**的影響力。

## 🥕🪓「訓政」影響力？

以上的分析，包括其**時間維度**與**空間維度**，說明了 LLM 模型在政治經濟上的「訓政」影響力。

特別是因RLHF學習過程，LLM 的訓練者與佈署者，會在兩個階段分別對 LLM 與使用者施加影響力，形成一種**雙階段引導機制**，其政治經濟意涵可概述如下：

* 🥕其「奬」是，訓練者與佈署者較為長期的**反思－符號心智**，使用者較為中短期的**反應－情緒心智**，因此有影響力不對稱的問題。
* 🪓其「罰」是不「奬」或不回，因此也有影響力不對稱的問題。
* 🏛️其「訓政」是， LLM 訓練者及佈署者是先行「訓導」與「訓練」LLM 學習和習慣某一種政治或政治意識型態，此階段的目標是**培養 LLM 的政治能力**，並在平台的引領下建立穩固的**人機對話體制** （含在模型之外的額外資訊處理機制）。
* ⚖️ **人機對話體制效應**：可能深化「憲政」，也可能強化「軍政」，可用來檢驗《推力》的「自由家長主義」假說，觀測其是否透過**不強迫**、**透明**且**合乎倫理**的手段，促進公共利益與民主參與。
    - 🌱 在「憲政」取向下，推力可用於提升公民知識水平、促進理性討論與多元觀點交流，讓人機互動成為民主治理的輔助工具。
    - 🛡️ 在「軍政」或集權取向下，推力則可能被用來強化單一敘事、壓制異議、塑造一致化的價值觀，最終削弱使用者的批判性思維與自主判斷。
    - 🔄 由於 LLM 的兩階段引導機制會在訓練者與使用者之間形成回饋循環，若缺乏外部監督與透明度，這種影響力可能在政治經濟結構中被制度化，成為長期且隱性的「影響力工程」。
* 🌍 **推力換勢力**：在地緣政治「勢力範圍」中，這種**人機對話體制**「推力」影響力可能跨越國界，滲透至不同文化與政治體系，成為國際競爭與話語權爭奪的重要工具。

總之，LLM 的「訓政」影響力揭示了人工智慧技術在當代政治經濟中的複雜角色，強調了在設計與部署這些系統時，必須謹慎考量其倫理、透明度與公共利益的維護。

