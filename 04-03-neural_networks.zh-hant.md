---
title: 🌀🪢🧠 神經網路
tags:
- 神經可塑性
- 神經元
- 連結主義
- 激勵函數
- 模型剪枝
- 深度學習
- 反向傳播
- 梯度下降
- 模式識別
- 圖像辨識
---
`神經網路`（Neural Networks）是種巧妙結合**數學**與**神經科學**的計算模型。它以生物學的**神經元結構**與**刺激與反應**機制為靈感，透過層層相連的**節點**（人工神經元）形成網絡，仿擬了人腦處理資訊的方式，並體現了**神經可塑性**（Neural Plasticity）的概念。神經網路的結構與演算法相輔相成，產生了強大的**非線性建模能力**，能夠自動從原始數據中**學習特徵**，解決複雜問題，成為一個既具備彈性又可擴展的智慧系統。

實現**學習**的具體數學機制，依賴**線性代數**處理複雜的神經元間運算，以及**微積分**支持的**反向傳播**（Backpropagation）和**梯度下降**（Gradient Descent）演算法。在訓練過程中，網路透過調整**權重**來最小化誤差。此時，**激勵函數**（Activation Function）扮演著至關重要的角色，它模擬生物神經元的「開關」或「決策者」，決定神經元是否被「激活」（fire）並傳遞訊號。這種模擬「刺激與反應」的層層激活機制，體現了「共同激發的神經元會彼此連結」（Neurons that fire together, wire together）的**[赫布學習法則](02-05-connectionism.zh-hant)**（Hebb's Law），是「統計流AI」[連結主義](02-05-connectionism.zh-hant)的體現。

隨著訓練的進行，**神經可塑性**的「用進廢退」（Use it or lose it）原則也在類神經網路中有所體現。**突觸（神經連結）剪枝**（Synaptic Pruning）的生物學概念，被應用於機器學習中的**模型剪枝**（Model Pruning）。這是一種透過移除貢獻度低的權重或神經元來優化網路效率的技術，能顯著減小模型大小、提高運算效率，並有效防止**過度擬合**（Overfitting），展現泛化能力。

類神經網路的分層架構，深受人腦處理視覺資訊的啟發。其中，**新認知機**（Neocognitron）是早期的一個重要里程碑，它開創性地提出了分層處理的架構，能從圖像中逐步提取特徵，類似於生物視覺皮層的工作原理。

## 🛅 闡明範例：圖像辨識

類神經網路最著名的應用之一是**圖像辨識**。**新認知機**分層處理的正是從圖像中逐步提取特徵，機器原來只看得到像素點的數值矩陣，而非人類能辨識的形狀與紋理，類神經網路正能彌補此差距。（參見 [完形心理](01-05-Gestalt_Psychology.zh-hant)。）

這種從簡單特徵（如邊緣）到複雜特徵（如形狀、物體）的層次化處理，為後來的**卷積神經網路**（CNN）等先進架構奠定了基礎，使其在圖像辨識等領域取得巨大成功。

以辨識手寫數字為例，一個類神經網路通常包含以下結構：

- 📥 **輸入層**（Input Layer）：由數百個節點組成，每個節點代表圖像中的一個像素。圖片的像素亮度值被送入對應節點。
    
- 🧠 **隱藏層**（Hidden Layers）：由多層節點構成，是進行「思考」的核心。淺層節點可能學習辨識直線或曲線等基本特徵；隨著資訊傳遞至更深層，節點會將這些基本特徵組合成更複雜的模式，例如「圓圈」或「橫槓」。
    
- 🔢 **輸出層**（Output Layer）：通常由數個節點組成，每個節點代表一個可能的分類結果。在手寫數字辨識中，可能有10個節點分別代表數字0到9。網路會根據隱藏層的處理結果，給出每個數字的機率，並最終選擇機率最高的作為辨識結果。
    
在整個過程中，網路會不斷透過**反向傳播**和**梯度下降**調整各層之間的**權重**，從大量的訓練圖片中學習，精準地將像素模式與正確的數字標籤連結起來。

## 🎋 類型

當代神經網路可依其**學習方式**與**資料處理特性**分為以下幾種典型架構：

- ▦ **多層感知器**（MLP）：
    - 最基本的前饋神經網路，資訊僅沿單一方向流動，無迴圈結構。
    - 適用於處理**靜態**、**非序列性的表格**或**結構化數據**，執行**分類**與**回歸**任務。
    - 難以捕捉時間序列或空間關聯性，不適合處理語音或影像等資料。
- 🧠 **卷積神經網路**（CNN）：
    - 受到人類**視覺皮層**啟發，透過⊛卷積層與⊗池化層自動提取局部空間特徵。
    - 在**圖像辨識**、**物件偵測**與**影片分析**等任務中表現卓越，能識別形狀、邊緣與紋理。
    - 權重共享與局部感受野設計有效降低參數量，提升訓練效率與泛化能力。
- 🔄 **遞歸神經網路**（RNN）：
    - 專為處理**序列資料**而設計，透過🔄遞歸連結與隱藏狀態⟳保留時間上下文。
    - 適用於**語音辨識**、**自然語言處理**與**時間序列**預測等任務。
    - 易受梯度消失或爆炸影響，在處理長序列時表現不穩定。
- 🧬 **長短期記憶網路**（LSTM）：
    - RNN 的改良版本，引入🔒門控機制（輸入門、遺忘門、輸出門）以控制資訊流。
    - 能**選擇性記憶或遺忘**資訊，有效捕捉長期依賴，克服**梯度消失**問題。
    - 廣泛應用於**機器翻譯**、**語音生成**與**複雜文本分析**等序列任務。
- 🧞‍♂️ **變換器**（Transformer）：
    - 完全捨棄遞歸結構，改用多頭自注意力機制來捕捉序列中的遠程依賴。
    - 可並行處理整個序列，大幅提升訓練效率與模型表現。
    - 為**大型語言模型**（LLM）如 GPT、BERT 等的核心架構，革新自然語言處理領域。

## 🌟 數學基礎與優勢

類神經網路的核心數學基礎是**線性代數**（用於高效的矩陣運算）和**微積分**（用於最佳化學習過程）。正是這些數學工具，讓網路能夠有效處理大量數據並解決複雜問題。
  
它的主要優勢在於：
- **🔀 高度彈性與可擴展性**：網路的層數和節點數可以根據問題的複雜度自由調整。增加層次後，便形成了能夠處理更抽象數據模式的**深度學習**模型。
- **🧠 自動學習特徵**：傳統機器學習方法通常需要工程師手動設計特徵，但類神經網路能直接從原始數據（例如監控影像的像素點）中自動辨識並提取有用的模式，這讓模型得以處理更廣泛、更複雜的數據。
- **📈 強大的非線性建模能力**：透過 **激勵函數** 與 **[梯度下降](09-02-steepest_descent_method.zh-hant)** ，網路能夠處理任何複雜的非線性關係，解決單純線性模型無法應對的現實問題。
	- **激勵函數** 如 `Sigmoid`、`ReLU` 或 `Tanh` 函數，接收前一層神經元的加權輸入總和，然後將其轉換為輸出訊號，決定神經元是否應該被「激發」。
	-  **梯度下降**（Gradient Descent）：神經網路的學習演算法，是**整個網路**的「學習者」。梯度下降是一種**最佳化演算法**。它的目標是最小化神經網路的預測誤差，也就是讓網路的輸出結果盡可能接近真實答案。

總之，神經網路利用其數學結構模擬人腦的神經運作方式，通過數據學習來解決複雜問題。

儘管如此，神經網路的局限性也很明顯。它需要龐大的數據與**GPU 算力**進行訓練，且由於其複雜的內部結構，通常被視為一個 **「黑箱」**。例如，當一個神經網路正確辨識出「威脅」或「異常」時，我們很難確切解釋它究竟是基於哪些像素、形狀或紋理組合來做出這個判斷，這使得它在需要高透明度的應用場景中面臨挑戰。

## 🧠🌲✂️ 剪枝與神經可塑性

類神經網路在學習過程中，其實也體現了生物學中神經可塑性（Neural Plasticity）的核心概念，特別是神經元剪枝（Synaptic Pruning）的行為。

* 用進廢退（Use-it-or-lose-it）：在生物學中，大腦會優先強化那些被頻繁使用的神經連結，同時修剪掉不常使用的連結以提高效率。在神經網路中，這與模型訓練過程中的權重最佳化非常相似。當網路不斷學習並更新權重時，那些對最終預測貢獻較大的權重會被強化；而對結果影響微弱的權重則會逐漸趨近於零，這可以被視為一種「廢棄」。
* 神經元剪枝（Neural Pruning）：當訓練好的神經網路模型過於龐大，包含大量冗餘或低效的權重時，機器學習工程師會使用模型剪枝（Model Pruning）技術。這個過程就像是人腦的發育：為了讓神經網路更輕量、更高效，我們會系統性地移除那些貢獻度極小的神經元或連接。這不僅能大幅減少模型的參數數量、降低運算需求，還能有效防止過度擬合（Overfitting），提升模型在未知數據上的泛化能力。

簡而言之，模型剪枝是將神經科學的用進廢退原則應用於類神經網路的實踐。透過這種方法，我們能夠創造出更精簡、更強大且更具成本效益的 AI 模型。

 
## 🪢🧠🔮 神經網路、深度學習與貝氏網路的關係

神經網路、深度學習和貝氏網路是 AI 模型中的三種關鍵思維，各自有不同的核心功能：

- 🪢**神經網路**：其核心是透過模仿人腦神經元的結構，自動從數據中學習並識別複雜的**模式**。
- 🧠 **深度學習**：這是一種特別強大的神經網路，因擁有許多隱藏層而能從海量數據中提取更抽象、更深層次的特徵。它的成功，主要得益於**GPU 算力**的普及，使複雜模型的訓練成為可能。
- 🔮 **貝氏網路**：其核心是**機率與因果推論**。它擅長處理不確定性問題，並提供每個判斷的機率依據。結合兩者優點的**貝氏深度學習**也應運而生，它為傳統深度學習提供了一種解決「黑箱」和「不確定性」問題的優雅方法。

簡而言之，**神經網路擅長「模式識別」**，而**貝氏網路擅長機率型的「因果推論」**。這兩種模型各有專精，並非相互取代。近年來，新興的**貝氏深度學習**領域，正是結合兩者優勢的嘗試，讓強大的深度學習模型也能像貝氏網路一樣，提供判斷的**信心水準**，使其應用在醫療診斷等高風險領域時更為可靠。

## ✨ 小結

類神經網路是現代 AI 最核心的技術之一。它從模仿人腦的簡單概念出發，透過強大的數學與演算法，發展成能自動學習、處理複雜數據的強大工具。它不僅推動了圖像、語音、自然語言處理等領域的革命，也為我們探索更深層次的 AI 提供了可能。

***

## 👉 接下來

- 回憶**統計流 AI**（Statistical AI）其它條目，評估自己可不可以說明**類神經網路**和他們的關係，如下所述：
	- **🌀🎲🌿 [機率性關聯](04-01-probabilistic_association.zh-hant)**：神經網路的學習過程，本質上就是在尋找輸入數據與輸出結果之間的**機率性關聯**。例如，它會學會一個像素組合（特徵）與特定數字（結果）之間有很高的關聯性。
	- **🌀🧞‍♀️🗪 [LLM聊天機器人](04-02-llm_chatbots.zh-hant)**：**大語言模型**（LLM）本身是一種強大的**神經網路**實例（instance）。...
	- **🌀🛠️🤏 [特徵工程](04-04-feature_engineering.zh-hant)**：神經網路的一大優勢是它能**自動學習特徵**，這大大減少了傳統機器學習中對**特徵工程**的依賴。在過去，人們必須手動設計要給模型的數據特徵，但神經網路能從原始數據中自己找到這些特徵。
	- **🌀🤖📦 [機器學習模型](04-05-machine_learning_models.zh-hant)**：神經網路是一種強大的**機器學習模型**實例（instance）。
	- **🌀🌐🔗 [大語言模型網組合](04-06-llm_webassembly.zh-hant.md)：**大語言模型**（LLM）本身是一種強大的**神經網路**實例（instance）。它們擁有龐大的層次和參數，因此能夠理解和生成複雜的文本。
	- 🌀🌌▦ **[向量空間](04-07-vector_space.zh-hant)**：神經網路無法直接處理字母或圖像等非數值資料，必須先將這些資料轉換為**向量**（`Vector`），也就是一組數字陣列，才能在由**線性代數**所定義的抽象**數學空間**中進行運算與學習。這由向量組成的抽象數學空間叫**向量空間**。
	    
- 回憶**符號流 AI**（Symbolic AI）所有條目，及其彼此關係，進而和**統計流 AI**進行參照比較。
- 思考 [第伍章 ☸](05----ai_orientations.zh-hant) AI 5 大導向（AI Orientations）的各種系統／設計思維視角，是如何運用 **符號流 AI** 或 **統計流 AI**為基礎，才能構成 **知識組織方式** 與 **問題解決策略**。