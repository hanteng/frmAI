---
tags:
- 神經可塑性
- 神經元
- 連結主義
- 激勵函數
- 模型剪枝
- 深度學習
- 反向傳播
- 梯度下降
- 模式識別
- 圖像辨識
- 完形心理
---

# 🌀🪢🧠 神經網路 {#sec-neural-networks}

`神經網路`（Neural Networks），或譯`類神經網路`，是一種巧妙結合**數學**與**神經科學**的計算模型，體現「統計流」AI 的 [連結主義](02-05-connectionism.zh-hant) 心智模型。靈感源自生物學的**神經元結構**（含**重複刺激強化突觸連結**機制），透過層層相連的**節點**（人工神經元）形成網絡。這種計算模型體現**神經可塑性**（Neural Plasticity），讓機器學習仿擬人腦處理神經元信號的方式，並透過經驗累積提升模式識別與泛化能力。

代表性案例包括現代的**大語言模型**（Large Language Models, LLMs）及其應用——LLM 聊天機器人，它們本質上都是以深度神經網路為基礎，透過龐大參數與多層結構實現自然語言的理解與生成。

作為**統計流 AI**（Statistical AI）的代表性里程碑，`神經網路` 革新了[機器學習](04-05-machine_learning_models.zh-hant)對**輸入與輸出之間關聯與誤差**的建模方式，顯著提升了模式識別與泛化能力。

## 🔼 神經元思考 🤔

`神經網路` 的革新性神經元思考能力，得利於**神經可塑性**，可歸納為三方面：

- 🧠 **生物啟發的學習原理**：`赫布學習法則`（Hebb's Law）  
  - 「共同激發的神經元會彼此連結」（Neurons that fire together, wire together）是 **[赫布學習法則](02-05-connectionism.zh-hant)** 的核心。人工神經網路借鑑此原理，根據輸入與輸出之間的關聯與誤差，持續調整神經元間的**連結強度**。
- 👨‍🔬 **動態學習的權重調整**：**反向傳播**（Backpropagation）與**梯度下降**（Gradient Descent）  
  - 透過**反向傳播**計算每個權重對誤差的貢獻，並用 [梯度下降](09-02-steepest_descent_method.zh-hant) 等最小化誤差演算法更新**權重**，模擬生物神經元在重複刺激下突觸連結的強化或削弱。結合**激勵函數**（Activation Function），決定神經元是否被激活並傳遞訊號（詳見 🪢👨‍🔬🧑🏻‍🏫 數學基礎 一節）。
- 💪🦾 **用進廢退與模型剪枝**  
  - 在**神經科學**中，「用進廢退」（Use it or lose it）指經常使用的神經連結會被強化，不常使用的則會被削弱甚至移除，這一過程稱為**突觸剪枝**（Synaptic Pruning）。  
  - 在**機器學習**中，對應的技術是**模型剪枝**（Model Pruning）：移除對輸出影響極小的權重或神經元，減少冗餘計算、降低運算成本並防止過度擬合，從而提升模型在未知數據上的泛化能力。

綜上，`神經網路` 能隨訓練資料的累積，自動優化特徵提取與訊號傳遞路徑，形成具可塑性且擁有強弱連結分佈的結構。模型剪枝正是將生物學的用進廢退原則應用於人工神經網路的實踐，讓 AI 模型更精簡、更高效且具成本效益。

## ▶️ 電腦視覺新認知機 🥸

**新認知機**（Neocognitron）由日本學者福島邦彥（Kunihiko Fukushima）於 1980 年提出，是電腦視覺發展史上的重要里程碑。它模擬哺乳動物視覺皮層的**分層處理**方式，透過多層結構逐步提取特徵：從簡單的邊緣與線條，到更高階的形狀與模式。新認知機仿擬視覺皮層的工作原理，開創性地提出分層處理架構，實現了類似於 [完形心理](01-05-Gestalt_Psychology.zh-hant) 的經驗法則學習。

這種設計不僅推動了**生物啟發式計算**（Biologically Inspired Computing）與**仿生計算**（Bionic Computing）的交叉研究，更證明了從生物系統汲取靈感能設計出高效的人工智慧模型。

新認知機的核心思想是將「刺激與反應」的激活機制結合**層層特徵提取**，形成能自動學習的結構。 📌 **應用示例**：以辨識手寫數字為例，類神經網路通常包含：

- 📥 **輸入層**：節點對應圖像像素值。
    
- 🧠 **隱藏層**：淺層學習直線或曲線等基本特徵，深層將其組合成更複雜模式。
    
- 🔢 **輸出層**：輸出分類機率最高的數字（0–9）。
    

這種架構賦予神經網路強大的**非線性建模能力**，使其能從原始數據中自動**學習特徵**，解決複雜的模式識別問題，並構成既具彈性又可擴展的智慧系統。在實務中，透過移除貢獻度低的神經元連結（模型剪枝），還能顯著減少模型大小、提升運算效率，並降低**過度擬合**（Overfitting）風險，進一步增強泛化能力。

這一理念直接為現代**卷積神經網路**（CNN）奠定基礎。CNN 延續了新認知機的分層處理思想，並引入**感受野**（Receptive Field）與**權值共享**（Weight Sharing）等機制：前者讓神經元僅關注輸入圖像的局部區域，後者則讓不同位置的神經元使用相同權重檢測相同特徵，從而大幅減少參數數量並提升訓練效率與泛化能力。

簡言之，新認知機開創了以分層特徵學習模仿人類視覺系統的道路，使現代深度學習模型能在圖像辨識、物件偵測與其他複雜視覺任務中展現卓越表現。

<a id="深度學習"></a>
## ⏩ 深度學習 🧐

`深度學習`（Deep Learning）是**神經網路**的一個重要子集，也是其進階形式。傳統神經網路可以是淺層（少數隱藏層）或深層（多層結構），而深度學習特指擁有**多個隱藏層**的神經網路，能透過層層非線性轉換，自動從原始數據中學習由低階到高階的抽象特徵。這種深層結構結合了**現代訓練技術**（如批次正規化、殘差連接、Dropout）與**高效硬體**（GPU/TPU），在影像、語音、自然語言等領域取得突破性成果。

現今大多數**深度學習**應用都是基於**深層神經網路**，並已成為生成式 AI、多模態系統等前沿技術的核心。深度學習的成功，來自於它能在無需大量人工特徵工程的情況下，直接從數據中學得複雜模式，並在多種任務中展現卓越的泛化能力。

### 🌲 深層神經網路

**深層神經網路**的主要優勢包括：

- 🔀 **高度彈性與可擴展性**：層數與節點數可依問題複雜度調整，層次加深後即可形成能處理更抽象數據模式的深度學習模型。
- 🧠 **自動學習特徵**：無需人工設計特徵，能直接從原始數據（如影像像素）中自動辨識並提取有用模式，適應更廣泛與複雜的資料型態。
- 📈 **強大的非線性建模能力**：可處理高度複雜的非線性關係，解決傳統線性模型無法應對的現實問題。

總之，**深層神經網路**模擬腦神經的訊號傳遞與學習方式，透過深度學習在多領域中解決複雜問題，並持續推動 AI 技術的邊界。

### 🎋 典型架構

當代神經網路可依**學習方式**與**資料處理特性**分為以下幾種典型架構。除了**多層感知器**（MLP）外（是否屬於深度學習取決於層數與規模），其餘架構幾乎總是深度學習：

- ▦ **多層感知器**（MLP）
    - 最基本的前饋神經網路，資訊僅沿單一方向流動，無迴圈結構。
    - 適用於處理**靜態**、**非序列性**的表格或結構化數據，執行**分類**與**回歸**任務。
    - 難以捕捉時間序列或空間關聯性，不適合處理語音或影像等資料。
- 🧠 **卷積神經網路**（CNN）
    - 受人類**視覺皮層**啟發，透過⊛卷積層與⊗池化層自動提取局部空間特徵。
    - 在**圖像辨識**、**物件偵測**、**影片分析**等任務中表現卓越，能識別形狀、邊緣與紋理。
    - 權重共享與局部感受野設計有效降低參數量，提升訓練效率與泛化能力。
- 🔄 **遞歸神經網路**（RNN）
    - 專為處理**序列資料**設計，透過🔄遞歸連結與隱藏狀態⟳保留時間上下文。
    - 適用於**語音辨識**、**自然語言處理**、**時間序列預測**等任務。
    - 易受梯度消失或爆炸影響，長序列處理表現不穩定。
- 🧬 **長短期記憶網路**（LSTM）
    - RNN 的改良版本，引入🔒門控機制（輸入門、遺忘門、輸出門）控制資訊流。
    - 能**選擇性記憶或遺忘**資訊，有效捕捉長期依賴，克服梯度消失問題。
    - 廣泛應用於**機器翻譯**、**語音生成**、**複雜文本分析**等序列任務。
- 🧞‍♂️ **變換器**（Transformer）
    - 捨棄遞歸結構，改用多頭自注意力機制捕捉序列中的遠程依賴。
    - 可並行處理整個序列，大幅提升訓練效率與模型表現。
    - 為**大型語言模型**（LLM）如 GPT、BERT 等的核心架構，革新自然語言處理領域。

###  🪢 數學基礎及算力要求

`深度學習`與`類神經網路`的核心數學基礎來自**線性代數**與**微積分**。線性代數負責處理神經元之間的運算，特別是權重矩陣與輸入向量的乘法，這是訊號在網路中傳遞與轉換的基礎。微積分則支撐了網路的學習過程，透過計算誤差函數對權重的偏導數（梯度），為權重更新提供方向與幅度。這一過程的關鍵在於 **反向傳播** 與 **[梯度下降](09-02-steepest_descent_method.zh-hant)**：前者將預測誤差由輸出層反向傳遞至輸入層，計算各層權重對誤差的貢獻；後者則依據梯度逐步調整權重，尋找使誤差最小化的參數組合。

在訊號傳遞過程中，**激勵函數**（Activation Function）扮演著不可或缺的角色。人工神經元會先對輸入訊號進行加權求和，再通過激勵函數決定是否「激活」並將訊號傳遞到下一層。激勵函數的主要作用是引入非線性，使神經網路能夠擬合與學習複雜的非線性關係。常見的激勵函數包括 `Sigmoid`、`ReLU` 與 `Tanh`，它們分別在梯度平滑性、計算效率與收斂特性上各有優勢。正因如此，深層神經網路才能在影像、語音與自然語言等高度複雜的資料中自動學習多層次的抽象特徵。

然而，這些數學與演算法的威力也伴隨著現實挑戰。深度神經網路的訓練通常需要龐大的標註數據與高效能的**GPU/TPU**運算資源，才能在合理時間內完成參數更新。此外，網路的多層結構與大量參數使其內部決策過程難以直接解釋，形成所謂的「**黑箱**」問題。在需要高透明度與可解釋性的應用場景（如醫療診斷、金融風控）中，這種不透明性可能成為限制，促使研究者探索模型壓縮、可解釋 AI 與混合推論等新方向（參見  [🆚對比貝氏網路🔮] 一節）。

## 🔄歷史演進🗿

`神經網路` 的發展歷程，從早期的生物啟發模型到今日的深度學習核心架構，見證了**統計流 AI**在模式識別與特徵學習上的巨大飛躍。

- 🤓**1943 年：McCulloch–Pitts 模型** ➠ 第一個形式化的人工神經元模型的提出，奠定了以數學邏輯模擬神經元運作的基礎。
- 🤠**1958 年：感知器** （Perceptron）➠由 Frank Rosenblatt 發明**單層感知器**，首次展示了機器可透過調整權重進行學習，但受限於線性可分問題。
- 🥸 **1980 年：新認知機**（Neocognitron）➠由 福島邦彥引入分層結構與局部感受野，啟發了後來的卷積神經網路（CNN），為**電腦視覺**奠基。
- 😁 **1986 年：反向傳播**（Backpropagation）➠由 Rumelhart 等人推廣普及多層神經網路的訓練方法，解決了單層感知器的限制，開啟**深層架構**的可行性。
- 😎 **2012 年：深度學習**（Deep Learning, DL）➠AlexNet 在 ImageNet 競賽中大幅領先，展示了深層 CNN 在大規模資料與 GPU 加速下的強大性能，展示深度學習突破。
- 🤗 **2020 年代：多模態與超大規模模型** ➠  Transformer 架構與[大語言模型](02-07-large_language_models.zh-hant)（LLM）將神經網路應用擴展至自然語言、圖像、語音等多模態領域，並在生成式 AI 中達到前所未有的流暢性與泛化能力。

由此可見，神經網路的歷史演進不僅推動了深度學習的興起，也為今日多模態與生成式 AI 的蓬勃發展奠定了堅實基礎。

## 🆚對比貝氏網路🔮

`神經網路` 不僅推動了深度學習的興起，也與另一條重要路線——**[貝氏網路](09-03-bayesian_network.zh-hant)**——形成了互補與對照。

- 👍 **優勢**
	- 😼 擅長**模式識別**與**特徵自動學習**，可從原始數據中經多層非線性轉換提取抽象表示。
	- 🕹 在影像、語音、自然語言等高維資料的處理上效率極高，能適應多模態與大規模數據場景。
	- 🚀 具備強大的表現力與泛化能力，能在無需大量人工特徵工程的情況下完成複雜任務。
- 👎 **局限**
	- 🕳 屬於**統計流 AI**，內部決策過程常被視為「黑箱」，可解釋性不足。
	- 🫣 難以直接處理因果推論與不確定性量化，對高透明度需求的應用（如醫療診斷）存在挑戰。
	- 😶‍🌫️ 訓練需大量數據與算力，對資源要求高。
- 🤝 互補趨勢：**貝氏深度學習**
	- 🔄 將神經網路的特徵學習能力與貝氏網路的**機率推論**與**不確定性估計**結合，既保留深度學習的高表現力，又提升模型在高風險應用中的可解釋性與可靠性。為傳統深度學習提供了一種解決「黑箱」和「不確定性」問題的優雅方法。
	- 🏥 在醫療、金融、科學研究等領域，這種融合方法可同時提供準確預測與決策信心評估。

簡而言之，**神經網路擅長「模式識別」**，而**貝氏網路擅長機率型的「因果推論」**。這兩種模型各有專精，並非相互取代。近年來，新興的**貝氏深度學習**領域，正是結合兩者優勢的嘗試，讓強大的深度學習模型也能像貝氏網路一樣，提供判斷的**信心水準**，使其應用在醫療診斷等高風險領域時更為可靠。

## 🎄 小結與展望 🎊

綜觀發展歷程，**神經網路**從最初的生物啟發模型，演進為支撐現代**深度學習**的核心技術，成功將數學理論、計算方法與神經科學概念融合，實現了從影像辨識、語音處理到自然語言理解的多領域突破。其關鍵優勢在於能自動學習特徵、建構多層抽象表示，並具備強大的非線性建模能力，讓 AI 能夠處理過去難以企及的複雜任務。

然而，神經網路的「黑箱」特性、對大量數據與算力的依賴，以及在因果推論與不確定性量化上的不足，仍是未來需要面對的挑戰。這也促使研究者探索**可解釋 AI**、**模型壓縮**、**混合推論**等方向，並嘗試與**貝氏網路**等符號流方法結合，形成兼具高表現力與高透明度的**貝氏深度學習**，以滿足醫療、金融等高風險領域對可靠性與可解釋性的需求。

展望未來，神經網路將持續在**多模態學習**、**生成式 AI**、**強化學習**與**邊緣運算**等領域深化應用，並在硬體加速、演算法優化與跨領域融合的推動下，邁向更高效、更智慧、更可信賴的發展階段。這不僅將拓展 AI 的應用邊界，也將推動人類在知識獲取、決策支持與創造力輔助上的全新可能。

***

## 👉接下來🪸

- ⮤✨ 對照 👁️⯊ [完形心理](01-05-Gestalt_Psychology.zh-hant) 中的圖像辨識判斷，思考**生物啟發式計算**與**仿生計算**的「腦補」機制。
- ⮤💫 接連🏮🧬 [連結主義](02-05-connectionism.zh-hant)，思考 **深度學習** 的本質為何。  
    - ⮤💫 對照🏮💪 [行為主義](02-06-behaviorism.zh-hant)，思考 **強化學習** 的本質為何。
- ⮤🎇 接連 😵‍💫🧞‍♀️ [大語言模型](02-07-large_language_models.zh-hant) 中的「接話」「補寫」動作，思考**生物啟發式計算**與**仿生計算**的推論方式。
- ⮦🚦 探究 **統計流 AI**（Statistical AI）其它條目，評估自己可否說明**類神經網路**與它們的關係，例如：
    - **🌀🎲🌿 [機率性關聯](04-01-probabilistic_association.zh-hant)**：神經網路的訓練過程，本質上是在學習輸入與輸出之間的**機率性關聯**。透過大量樣本，網路會調整權重，使特定輸入模式對應到高機率的正確輸出，例如某組像素特徵對應到特定數字或物件。
    - **🌀🧞‍♀️🗪 [LLM 聊天機器人](04-02-llm_chatbots.zh-hant)**：**大語言模型**（LLM）是深層神經網路的代表性應用之一。它們透過龐大的參數量與多層結構，學會在自然語言中捕捉語意、上下文與語法規律，生成流暢且具邏輯性的回應。
    - **🌀🛠️🤏 [特徵工程](04-04-feature_engineering.zh-hant)**：神經網路的一大優勢是能**自動學習特徵**，大幅減少傳統機器學習中對人工**特徵工程**的依賴。過去需要專家手動設計輸入特徵，如今網路可直接從原始數據（影像像素、聲音波形、文字序列）中自行提取多層次的有用表示。
    - **🌀🤖📦 [機器學習模型](04-05-machine_learning_models.zh-hant)**：神經網路是機器學習模型的一種強大實踐方法，特別適合處理高維度、非線性且結構複雜的資料，並在分類、回歸、生成等任務中展現優勢。
    - **🌀🌐🔗 [大語言模型網組合](04-06-llm_webassembly.zh-hant)**：**大語言模型**（LLM）不僅是深層神經網路的實例，還能與 **WebAssembly** 結合，在瀏覽器等多平台運行智慧應用。
    - **🌀🌌▦ [向量空間](04-07-vector_space.zh-hant)**：神經網路無法直接處理文字、圖像等非數值資料，必須先將其轉換為**向量**（`Vector`）——一組數字陣列。這些向量存在於由**線性代數**定義的抽象**數學空間**中，網路便能在此空間中進行運算與學習，捕捉資料間的幾何與語意關係。
