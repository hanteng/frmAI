---
title: 🌀🪢🧠 神經網路
tags:
- 神經可塑性
- 神經元
- 連結主義
- 激勵函數
- 模型剪枝
- 深度學習
- 反向傳播
- 梯度下降
- 模式識別
- 圖像辨識
---
`神經網路`（Neural Networks），或譯`類神經網路`，是種巧妙結合**數學**與**神經科學**的計算模型。它以生物學的**神經元結構**與**刺激與反應**機制為靈感，透過層層相連的**節點**（人工神經元）形成網絡，仿擬了人腦處理資訊的方式，並體現**神經可塑性**（Neural Plasticity）的機器學習構造過程。

🧠`赫布學習法則`➠ 為體現了 **[赫布學習法則](02-05-connectionism.zh-hant)**（Hebb's Law）的「共同激發的神經元會彼此連結」（Neurons that fire together, wire together），神經網路的具體**學習**機制，整合最小化誤差**演算法**與**神經元激活**的**激勵函數**（Activation Function）。這種模擬「刺激與反應」的激活機制，體現「統計流AI」的 [連結主義](02-05-connectionism.zh-hant) 心智現象模型，受**新認知機**（Neocognitron）的**層層**結構啟發，把分層處理的架構用於電腦視覺創新。

🪢`用進廢退`的 神經可塑性➠同時體現了**神經可塑性**的「用進廢退」（Use it or lose it）原則，神經網路在機器學習中的**模型剪枝**（Model Pruning），隨著訓練的進行，好比於生物學**突觸（神經連結）剪枝**（Synaptic Pruning）的概念。透過移除貢獻度低的神經元連結來優化網路的技術，能顯著減小模型大小、提高運算效率，並有效防止**過度擬合**（Overfitting），展現泛化能力。

神經網路的結構與演算法相輔相成，產生了強大的**非線性建模能力**，能從原始數據中自動**學習特徵**解決複雜問題，構成既具備彈性又可擴展的智慧系統。

## 🛅 闡明範例：電腦視覺的新認知機

對人工智能和機器學習領域，特別是電腦視覺（Computer Vision）的發展產生深遠影響的**新認知機**（Neocognitron），是由日本學者福島邦彦（Kunihiko Fukushima）於1980年提出。

新認知機透過模擬哺乳動物視覺皮層的**分層處理**方式，逐步提取從簡單邊緣、線條到高階形狀的層次化特徵。這種思想不僅推動了**生物啟發式計算**（Biologically Inspired Computing）或**仿生計算**（Bionic Computing）的交叉研究，更證明了從生物系統汲取靈感能設計出高效模型。

簡言之，神經網路著名應用之一，正是從圖像中逐步提取特徵的電腦視覺特徵學習，能層次化學習模仿人類視覺系統，如形狀與紋理，是現代**深度學習**模型能夠處理複雜**視覺任務**的關鍵。

***

以辨識手寫數字為例，類神經網路通常包含：

- 📥 **輸入層**：代表圖像的像素值，每個節點代表圖像的一個像素。
- 🧠 **隱藏層**：多層節點，淺層學習基本特徵（如直線或曲線），深層則將其組合成更複雜模式。
- 🔢 **輸出層**：代表可能的分類結果（如數字0-9），輸出機率最高的即為辨識結果。

新認知機仿擬視覺皮層的工作原理，開創性地提出分層處理架構，實現了類似於 [完形心理](01-05-Gestalt_Psychology.zh-hant) 的經驗法則學習。

新認知機更現代為**卷積神經網絡**（CNN）奠基，透過**感受野**（Receptive Field）與**權值共享**（Weight Sharing）等機制，為後來的AlexNet、VGG、ResNet等CNN架構奠定了基礎。**感受野**（Receptive Field）是指神經元在處理視覺資訊時，僅關注輸入圖像中的特定局部區域。**權值共享**（Weight Sharing）則是讓網絡中的不同神經元使用相同的權重來檢測圖像中各處的相同特徵，從而大幅減少模型的參數數量，提高訓練效率與泛化能力。

下兩節分別描述神經網路的**數學基礎**與**典型架構**。

##  🪢👨‍🔬🧑🏻‍🏫 數學基礎

類神經網路之所以能具備強大的**非線性建模能力**，並從原始數據中自動**學習特徵**來解決複雜問題，其核心數學基礎是**線性代數**和**微積分**。**線性代數**負責處理神經元之間的複雜運算，特別是權重矩陣的乘法。**微積分**則透過**反向傳播**（Back propagation）和**梯度下降**（Gradient Descent）演算法，實現學習。

**反向傳播**是將預測誤差從輸出層反向傳遞至輸入層的機制，以便計算每一層權重的貢獻。**梯度下降**則是一種重要的**最佳化演算法**，其目標是逐步找到使網路誤差最小化的權重組合。透過計算誤差函數對權重的**梯度**（即變化率），網路得知調整權重的方向，從而向正確答案靠近。

在訓練過程中，類神經網路透過不斷調整**權重**來最小化預測誤差。這時，**激勵函數**（Activation Function）扮演著關鍵角色，它模擬生物神經元的「開關」功能，決定神經元是否被「激活」並傳遞訊號。常見的激勵函數包括 `Sigmoid`、`ReLU` 和 `Tanh`。

以上數學及計算基礎，讓類神經網路能夠從龐大數據中學習模式。然而，神經網路也面臨挑戰，例如需要大量的訓練數據與**GPU 算力**，且其複雜的內部結構常被視為「**黑箱**」，難以精確解釋其決策依據，這在需要高透明度的應用場景中可能構成限制。

## 🎋 典型架構

當代神經網路可依其**學習方式**與**資料處理特性**分為以下幾種典型架構：

- ▦ **多層感知器**（MLP）：
    - 最基本的前饋神經網路，資訊僅沿單一方向流動，無迴圈結構。
    - 適用於處理**靜態**、**非序列性的表格**或**結構化數據**，執行**分類**與**回歸**任務。
    - 難以捕捉時間序列或空間關聯性，不適合處理語音或影像等資料。
- 🧠 **卷積神經網路**（CNN）：
    - 受到人類**視覺皮層**啟發，透過⊛卷積層與⊗池化層自動提取局部空間特徵。
    - 在**圖像辨識**、**物件偵測**與**影片分析**等任務中表現卓越，能識別形狀、邊緣與紋理。
    - 權重共享與局部感受野設計有效降低參數量，提升訓練效率與泛化能力。
- 🔄 **遞歸神經網路**（RNN）：
    - 專為處理**序列資料**而設計，透過🔄遞歸連結與隱藏狀態⟳保留時間上下文。
    - 適用於**語音辨識**、**自然語言處理**與**時間序列**預測等任務。
    - 易受梯度消失或爆炸影響，在處理長序列時表現不穩定。
- 🧬 **長短期記憶網路**（LSTM）：
    - RNN 的改良版本，引入🔒門控機制（輸入門、遺忘門、輸出門）以控制資訊流。
    - 能**選擇性記憶或遺忘**資訊，有效捕捉長期依賴，克服**梯度消失**問題。
    - 廣泛應用於**機器翻譯**、**語音生成**與**複雜文本分析**等序列任務。
- 🧞‍♂️ **變換器**（Transformer）：
    - 完全捨棄遞歸結構，改用多頭自注意力機制來捕捉序列中的遠程依賴。
    - 可並行處理整個序列，大幅提升訓練效率與模型表現。
    - 為**大型語言模型**（LLM）如 GPT、BERT 等的核心架構，革新自然語言處理領域。

以上各架構都體現 **[赫布學習法則](02-05-connectionism.zh-hant)**，並以不同的方式利用以下神經神經網路的**主要優勢**：

- **🔀 高度彈性與可擴展性**：網路的層數和節點數可以根據問題的複雜度自由調整。增加層次後，便形成了能夠處理更抽象數據模式的**深度學習**模型。
    
- **🧠 自動學習特徵**：傳統機器學習方法通常需要工程師手動設計特徵，但類神經網路能直接從原始數據（例如監控影像的像素點）中自動辨識並提取有用的模式，這讓模型得以處理更廣泛、更複雜的數據。
    
- **📈 強大的非線性建模能力**：類神經網路能夠處理任何複雜的非線性關係，解決單純線性模型無法應對的現實問題。
    

總之，神經網路利用其數學結構模擬人腦的神經運作方式，通過數據學習來解決複雜問題。

## 🪢✂️🎍 神經可塑性的剪枝

類神經網路在學習過程中，其實也體現了生物學中神經可塑性（Neural Plasticity）的核心概念，特別是神經元剪枝（Synaptic Pruning）的行為。

- 💪🦾 **用進廢退**（Use-it-or-lose-it）：在生物學中，大腦會優先強化那些被頻繁使用的神經連結，同時修剪掉不常使用的連結以提高效率。在神經網路中，這與模型訓練過程中的權重最佳化非常相似。當網路不斷學習並更新權重時，那些對最終預測貢獻較大的權重會被強化；而對結果影響微弱的權重則會逐漸趨近於零，這可以被視為一種「廢棄」。
    
- ✂️🎍 神經元剪枝（Neural Pruning）：當訓練好的神經網路模型過於龐大，包含大量冗餘或低效的權重時，機器學習工程師會使用模型剪枝（Model Pruning）技術。這個過程就像是人腦的發育：為了讓神經網路更輕量、更高效，我們會系統性地移除那些貢獻度極小的神經元或連接。這不僅能大幅減少模型的參數數量、降低運算需求，還能有效防止過度擬合（Overfitting），提升模型在未知數據上的泛化能力。
    

簡而言之，模型剪枝是將神經科學的用進廢退原則應用於類神經網路的實踐。透過這種方法，我們能夠創造出更精簡、更強大且更具成本效益的 AI 模型。

## 🪢🧠🔮 神經網路、深度學習與貝氏網路的關係

神經網路、深度學習和貝氏網路是 AI 模型中的三種關鍵思維，各自有不同的核心功能：

- 🪢**神經網路**：其核心是透過模仿人腦神經元的結構，自動從數據中學習並識別複雜的**模式**。
    
- 🧠 **深度學習**：這是一種特別強大的神經網路，因擁有許多隱藏層而能從海量數據中提取更抽象、更深層次的特徵。它的成功，主要得益於**GPU 算力**的普及，使複雜模型的訓練成為可能。
    
- 🔮 **貝氏網路**：其核心是**機率與因果推論**。它擅長處理不確定性問題，並提供每個判斷的機率依據。結合兩者優點的**貝氏深度學習**也應運而生，它為傳統深度學習提供了一種解決「黑箱」和「不確定性」問題的優雅方法。
    

簡而言之，**神經網路擅長「模式識別」**，而**貝氏網路擅長機率型的「因果推論」**。這兩種模型各有專精，並非相互取代。近年來，新興的**貝氏深度學習**領域，正是結合兩者優勢的嘗試，讓強大的深度學習模型也能像貝氏網路一樣，提供判斷的**信心水準**，使其應用在醫療診斷等高風險領域時更為可靠。

## ✨ 小結 🏁

類神經網路是現代 AI 的核心技術之一。它從模仿人腦神經元的簡單概念出發，透過強大的數學與演算法，發展成能自動學習、處理複雜數據的強大工具。**深度學習**作為其一個分支，得益於強大的 GPU 算力，能夠處理更抽象、更深層次的特徵。雖然神經網路擅長**模式識別**，但其「黑箱」特性仍是一大挑戰。因此，結合了**貝氏網路**的**貝氏深度學習**，正在嘗試為模型提供**機率與因果推論**能力，讓 AI 的決策過程更具透明度與可靠性。

***

## 👉 接下來

- 回憶**統計流 AI**（Statistical AI）其它條目，評估自己可不可以說明**類神經網路**和他們的關係，如下所述：
	- **🌀🎲🌿 [機率性關聯](04-01-probabilistic_association.zh-hant)**：神經網路的學習過程，本質上就是在尋找輸入數據與輸出結果之間的**機率性關聯**。例如，它會學會一個像素組合（特徵）與特定數字（結果）之間有很高的關聯性。
	- **🌀🧞‍♀️🗪 [LLM聊天機器人](04-02-llm_chatbots.zh-hant)**：**大語言模型**（LLM）本身是一種強大的**神經網路**實例（instance）。...
	- **🌀🛠️🤏 [特徵工程](04-04-feature_engineering.zh-hant)**：神經網路的一大優勢是它能**自動學習特徵**，這大大減少了傳統機器學習中對**特徵工程**的依賴。在過去，人們必須手動設計要給模型的數據特徵，但神經網路能從原始數據中自己找到這些特徵。
	- **🌀🤖📦 [機器學習模型](04-05-machine_learning_models.zh-hant)**：神經網路是一種強大的**機器學習模型**實例（instance）。
	- **🌀🌐🔗 [大語言模型網組合](04-06-llm_webassembly.zh-hant.md)：**大語言模型**（LLM）本身是一種強大的**神經網路**實例（instance）。它們擁有龐大的層次和參數，因此能夠理解和生成複雜的文本。
	- 🌀🌌▦ **[向量空間](04-07-vector_space.zh-hant)**：神經網路無法直接處理字母或圖像等非數值資料，必須先將這些資料轉換為**向量**（`Vector`），也就是一組數字陣列，才能在由**線性代數**所定義的抽象**數學空間**中進行運算與學習。這由向量組成的抽象數學空間叫**向量空間**。
	    
- 回憶**符號流 AI**（Symbolic AI）所有條目，及其彼此關係，進而和**統計流 AI**進行參照比較。
- 思考 [第伍章 ☸](05----ai_orientations.zh-hant) AI 5 大導向（AI Orientations）的各種系統／設計思維視角，是如何運用 **符號流 AI** 或 **統計流 AI**為基礎，才能構成 **知識組織方式** 與 **問題解決策略**。