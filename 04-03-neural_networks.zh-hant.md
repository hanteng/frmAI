---
title: 🌀🪢🧠 神經網路
tags:
- 神經可塑性
- 神經元
- 連結主義
- 激勵函數
- 模型剪枝
- 深度學習
- 反向傳播
- 梯度下降
- 模式識別
- 圖像辨識
---
`神經網路`（Neural Networks），或譯`類神經網路`，是一種巧妙結合**數學**與**神經科學**的計算模型，體現「統計流」AI 的 [連結主義](02-05-connectionism.zh-hant) 心智模型。靈感源自生物學的**神經元結構**（含**重複刺激強化突觸連結**機制），透過層層相連的**節點**（人工神經元）形成網絡。這種計算模型體現**神經可塑性**（Neural Plasticity），讓機器學習仿擬人腦處理神經元信號的方式，並透過經驗累積提升模式識別與泛化能力。

代表性案例包括現代的**大語言模型**（Large Language Models, LLMs）及其應用——LLM 聊天機器人，它們本質上都是以深度神經網路為基礎，透過龐大參數與多層結構實現自然語言的理解與生成。

作為**統計流 AI**（Statistical AI）的代表性里程碑，`神經網路` 革新了[機器學習](04-05-machine_learning_models.zh-hant)對**輸入與輸出之間關聯與誤差**的建模方式，顯著提升了模式識別與泛化能力。

## 🔼 神經元思考 🤔

`神經網路` 的革新性神經元思考能力，得利於**神經可塑性**，可歸納為三方面：

- 🧠 **生物啟發的學習原理**：`赫布學習法則`（Hebb's Law）  
  - 「共同激發的神經元會彼此連結」（Neurons that fire together, wire together）是 **[赫布學習法則](02-05-connectionism.zh-hant)** 的核心。人工神經網路借鑑此原理，根據輸入與輸出之間的關聯與誤差，持續調整神經元間的**連結強度**。
- 👨‍🔬 **動態學習的權重調整**：**反向傳播**（Backpropagation）與**梯度下降**（Gradient Descent）  
  - 透過**反向傳播**計算每個權重對誤差的貢獻，並用 [梯度下降](09-02-steepest_descent_method.zh-hant) 等最小化誤差演算法更新**權重**，模擬生物神經元在重複刺激下突觸連結的強化或削弱。結合**激勵函數**（Activation Function），決定神經元是否被激活並傳遞訊號（詳見 🪢👨‍🔬🧑🏻‍🏫 數學基礎 一節）。
- 💪🦾 **用進廢退與模型剪枝**  
  - 在**神經科學**中，「用進廢退」（Use it or lose it）指經常使用的神經連結會被強化，不常使用的則會被削弱甚至移除，這一過程稱為**突觸剪枝**（Synaptic Pruning）。  
  - 在**機器學習**中，對應的技術是**模型剪枝**（Model Pruning）：移除對輸出影響極小的權重或神經元，減少冗餘計算、降低運算成本並防止過度擬合，從而提升模型在未知數據上的泛化能力。

綜上，`神經網路` 能隨訓練資料的累積，自動優化特徵提取與訊號傳遞路徑，形成具可塑性且擁有強弱連結分佈的結構。模型剪枝正是將生物學的用進廢退原則應用於人工神經網路的實踐，讓 AI 模型更精簡、更高效且具成本效益。

## ▶️ 電腦視覺新認知機 🥸


這種模擬「刺激與反應」的激活機制，，受**新認知機**（Neocognitron）的**層層**結構啟發，把分層處理的架構用於電腦視覺創新。



透過移除貢獻度低的神經元連結來優化網路的技術，能顯著減小模型大小、提高運算效率，並有效防止**過度擬合**（Overfitting），展現泛化能力。

神經網路的結構與演算法相輔相成，產生了強大的**非線性建模能力**，能從原始數據中自動**學習特徵**解決複雜問題，構成既具備彈性又可擴展的智慧系統。


對人工智能和機器學習領域，特別是電腦視覺（Computer Vision）的發展產生深遠影響的**新認知機**（Neocognitron），是由日本學者福島邦彦（Kunihiko Fukushima）於1980年提出。

新認知機透過模擬哺乳動物視覺皮層的**分層處理**方式，逐步提取從簡單邊緣、線條到高階形狀的層次化特徵。這種思想不僅推動了**生物啟發式計算**（Biologically Inspired Computing）或**仿生計算**（Bionic Computing）的交叉研究，更證明了從生物系統汲取靈感能設計出高效模型。

簡言之，神經網路著名應用之一，正是從圖像中逐步提取特徵的電腦視覺特徵學習，能層次化學習模仿人類視覺系統，如形狀與紋理，是現代**深度學習**模型能夠處理複雜**視覺任務**的關鍵。

***

以辨識手寫數字為例，類神經網路通常包含：

- 📥 **輸入層**：代表圖像的像素值，每個節點代表圖像的一個像素。
- 🧠 **隱藏層**：多層節點，淺層學習基本特徵（如直線或曲線），深層則將其組合成更複雜模式。
- 🔢 **輸出層**：代表可能的分類結果（如數字0-9），輸出機率最高的即為辨識結果。

新認知機仿擬視覺皮層的工作原理，開創性地提出分層處理架構，實現了類似於 [完形心理](01-05-Gestalt_Psychology.zh-hant) 的經驗法則學習。

新認知機更現代為**卷積神經網絡**（CNN）奠基，透過**感受野**（Receptive Field）與**權值共享**（Weight Sharing）等機制，為後來的AlexNet、VGG、ResNet等CNN架構奠定了基礎。**感受野**（Receptive Field）是指神經元在處理視覺資訊時，僅關注輸入圖像中的特定局部區域。**權值共享**（Weight Sharing）則是讓網絡中的不同神經元使用相同的權重來檢測圖像中各處的相同特徵，從而大幅減少模型的參數數量，提高訓練效率與泛化能力。

*** 

<a id="深度學習"></a>
## ⏩ 深度學習 🧐

`深度學習`（Deep Learning）是**神經網路**的一個重要子集，也是其進階形式。傳統神經網路可以是淺層（少數隱藏層）或深層（多層結構），而深度學習特指擁有**多個隱藏層**的神經網路，能透過層層非線性轉換，自動從原始數據中學習由低階到高階的抽象特徵。這種深層結構結合了**現代訓練技術**（如批次正規化、殘差連接、Dropout）與**高效硬體**（GPU/TPU），在影像、語音、自然語言等領域取得突破性成果。

現今大多數**深度學習**應用都是基於**深層神經網路**，並已成為生成式 AI、多模態系統等前沿技術的核心。深度學習的成功，來自於它能在無需大量人工特徵工程的情況下，直接從數據中學得複雜模式，並在多種任務中展現卓越的泛化能力。

### 🌲 深層神經網路

**深層神經網路**的主要優勢包括：

- 🔀 **高度彈性與可擴展性**：層數與節點數可依問題複雜度調整，層次加深後即可形成能處理更抽象數據模式的深度學習模型。
- 🧠 **自動學習特徵**：無需人工設計特徵，能直接從原始數據（如影像像素）中自動辨識並提取有用模式，適應更廣泛與複雜的資料型態。
- 📈 **強大的非線性建模能力**：可處理高度複雜的非線性關係，解決傳統線性模型無法應對的現實問題。

總之，**深層神經網路**模擬腦神經的訊號傳遞與學習方式，透過深度學習在多領域中解決複雜問題，並持續推動 AI 技術的邊界。

### 🎋 典型架構

當代神經網路可依**學習方式**與**資料處理特性**分為以下幾種典型架構。除了**多層感知器**（MLP）外（是否屬於深度學習取決於層數與規模），其餘架構幾乎總是深度學習：

- ▦ **多層感知器**（MLP）
    - 最基本的前饋神經網路，資訊僅沿單一方向流動，無迴圈結構。
    - 適用於處理**靜態**、**非序列性**的表格或結構化數據，執行**分類**與**回歸**任務。
    - 難以捕捉時間序列或空間關聯性，不適合處理語音或影像等資料。
- 🧠 **卷積神經網路**（CNN）
    - 受人類**視覺皮層**啟發，透過⊛卷積層與⊗池化層自動提取局部空間特徵。
    - 在**圖像辨識**、**物件偵測**、**影片分析**等任務中表現卓越，能識別形狀、邊緣與紋理。
    - 權重共享與局部感受野設計有效降低參數量，提升訓練效率與泛化能力。
- 🔄 **遞歸神經網路**（RNN）
    - 專為處理**序列資料**設計，透過🔄遞歸連結與隱藏狀態⟳保留時間上下文。
    - 適用於**語音辨識**、**自然語言處理**、**時間序列預測**等任務。
    - 易受梯度消失或爆炸影響，長序列處理表現不穩定。
- 🧬 **長短期記憶網路**（LSTM）
    - RNN 的改良版本，引入🔒門控機制（輸入門、遺忘門、輸出門）控制資訊流。
    - 能**選擇性記憶或遺忘**資訊，有效捕捉長期依賴，克服梯度消失問題。
    - 廣泛應用於**機器翻譯**、**語音生成**、**複雜文本分析**等序列任務。
- 🧞‍♂️ **變換器**（Transformer）
    - 捨棄遞歸結構，改用多頭自注意力機制捕捉序列中的遠程依賴。
    - 可並行處理整個序列，大幅提升訓練效率與模型表現。
    - 為**大型語言模型**（LLM）如 GPT、BERT 等的核心架構，革新自然語言處理領域。

###  🪢 數學基礎及算力要求

`深度學習`與`類神經網路`的核心數學基礎來自**線性代數**與**微積分**。線性代數負責處理神經元之間的運算，特別是權重矩陣與輸入向量的乘法，這是訊號在網路中傳遞與轉換的基礎。微積分則支撐了網路的學習過程，透過計算誤差函數對權重的偏導數（梯度），為權重更新提供方向與幅度。這一過程的關鍵在於 **反向傳播** 與 **[梯度下降](09-02-steepest_descent_method.zh-hant)**：前者將預測誤差由輸出層反向傳遞至輸入層，計算各層權重對誤差的貢獻；後者則依據梯度逐步調整權重，尋找使誤差最小化的參數組合。

在訊號傳遞過程中，**激勵函數**（Activation Function）扮演著不可或缺的角色。人工神經元會先對輸入訊號進行加權求和，再通過激勵函數決定是否「激活」並將訊號傳遞到下一層。激勵函數的主要作用是引入非線性，使神經網路能夠擬合與學習複雜的非線性關係。常見的激勵函數包括 `Sigmoid`、`ReLU` 與 `Tanh`，它們分別在梯度平滑性、計算效率與收斂特性上各有優勢。正因如此，深層神經網路才能在影像、語音與自然語言等高度複雜的資料中自動學習多層次的抽象特徵。

然而，這些數學與演算法的威力也伴隨著現實挑戰。深度神經網路的訓練通常需要龐大的標註數據與高效能的**GPU/TPU**運算資源，才能在合理時間內完成參數更新。此外，網路的多層結構與大量參數使其內部決策過程難以直接解釋，形成所謂的「**黑箱**」問題。在需要高透明度與可解釋性的應用場景（如醫療診斷、金融風控）中，這種不透明性可能成為限制，促使研究者探索模型壓縮、可解釋 AI 與混合推論等新方向（參見  [🆚對比貝氏網路🔮] 一節）。

## 🔄 歷史演進 🗿

`神經網路` 的發展歷程，從早期的生物啟發模型到今日的深度學習核心架構，見證了**統計流 AI**在模式識別與特徵學習上的巨大飛躍。

- 🤓**1943 年：McCulloch–Pitts 模型** ➠ 第一個形式化的人工神經元模型的提出，奠定了以數學邏輯模擬神經元運作的基礎。
- 🤠**1958 年：感知器** （Perceptron）➠由 Frank Rosenblatt 發明**單層感知器**，首次展示了機器可透過調整權重進行學習，但受限於線性可分問題。
- 🥸 **1980 年：新認知機**（Neocognitron）➠由 福島邦彥引入分層結構與局部感受野，啟發了後來的卷積神經網路（CNN），為**電腦視覺**奠基。
- 😁 **1986 年：反向傳播**（Backpropagation）➠由 Rumelhart 等人推廣普及多層神經網路的訓練方法，解決了單層感知器的限制，開啟**深層架構**的可行性。
- 😎 **2012 年：深度學習**（Deep Learning, DL）➠AlexNet 在 ImageNet 競賽中大幅領先，展示了深層 CNN 在大規模資料與 GPU 加速下的強大性能，展示深度學習突破。
- 🤗 **2020 年代：多模態與超大規模模型** ➠  Transformer 架構與[大語言模型](02-07-large_language_models.zh-hant)（LLM）將神經網路應用擴展至自然語言、圖像、語音等多模態領域，並在生成式 AI 中達到前所未有的流暢性與泛化能力。

## 🆚對比貝氏網路🔮

`神經網路` 不僅推動了深度學習的興起，也與另一條重要路線——**[貝氏網路](09-03-bayesian_network.zh-hant)**——形成了互補與對照。

- 👍 **優勢**
	- 😼 擅長**模式識別**與**特徵自動學習**，可從原始數據中經多層非線性轉換提取抽象表示。
	- 🕹 在影像、語音、自然語言等高維資料的處理上效率極高，能適應多模態與大規模數據場景。
	- 🚀 具備強大的表現力與泛化能力，能在無需大量人工特徵工程的情況下完成複雜任務。
- 👎 **限制**
	- 🕳 屬於**統計流 AI**，內部決策過程常被視為「黑箱」，可解釋性不足。
	- 🫣 難以直接處理因果推論與不確定性量化，對高透明度需求的應用（如醫療診斷）存在挑戰。
	- 😶‍🌫️ 訓練需大量數據與算力，對資源要求高。
- 🤝 互補趨勢：**貝氏深度學習**
	- 🔄 將神經網路的特徵學習能力與貝氏網路的**機率推論**與**不確定性估計**結合，既保留深度學習的高表現力，又提升模型在高風險應用中的可解釋性與可靠性。為傳統深度學習提供了一種解決「黑箱」和「不確定性」問題的優雅方法。
	- 🏥 在醫療、金融、科學研究等領域，這種融合方法可同時提供準確預測與決策信心評估。

簡而言之，**神經網路擅長「模式識別」**，而**貝氏網路擅長機率型的「因果推論」**。這兩種模型各有專精，並非相互取代。近年來，新興的**貝氏深度學習**領域，正是結合兩者優勢的嘗試，讓強大的深度學習模型也能像貝氏網路一樣，提供判斷的**信心水準**，使其應用在醫療診斷等高風險領域時更為可靠。

## ✨ 小結 🏁

類神經網路是現代 AI 的核心技術之一。它從模仿人腦神經元的簡單概念出發，透過強大的數學與演算法，發展成能自動學習、處理複雜數據的強大工具。**深度學習**作為其一個分支，得益於強大的 GPU 算力，能夠處理更抽象、更深層次的特徵。雖然神經網路擅長**模式識別**，但其「黑箱」特性仍是一大挑戰。因此，結合了**貝氏網路**的**貝氏深度學習**，正在嘗試為模型提供**機率與因果推論**能力，讓 AI 的決策過程更具透明度與可靠性。

***

## 👉 接下來

- 回憶**統計流 AI**（Statistical AI）其它條目，評估自己可不可以說明**類神經網路**和他們的關係，如下所述：
	- **🌀🎲🌿 [機率性關聯](04-01-probabilistic_association.zh-hant)**：神經網路的學習過程，本質上就是在尋找輸入數據與輸出結果之間的**機率性關聯**。例如，它會學會一個像素組合（特徵）與特定數字（結果）之間有很高的關聯性。
	- **🌀🧞‍♀️🗪 [LLM聊天機器人](04-02-llm_chatbots.zh-hant)**：**大語言模型**（LLM）本身是一種強大的**神經網路**實例（instance）。...
	- **🌀🛠️🤏 [特徵工程](04-04-feature_engineering.zh-hant)**：神經網路的一大優勢是它能**自動學習特徵**，這大大減少了傳統機器學習中對**特徵工程**的依賴。在過去，人們必須手動設計要給模型的數據特徵，但神經網路能從原始數據中自己找到這些特徵。
	- **🌀🤖📦 [機器學習模型](04-05-machine_learning_models.zh-hant)**：神經網路是一種強大的**機器學習模型**實例（instance）。
	- **🌀🌐🔗 [大語言模型網組合](04-06-llm_webassembly.zh-hant.md)：**大語言模型**（LLM）本身是一種強大的**神經網路**實例（instance）。它們擁有龐大的層次和參數，因此能夠理解和生成複雜的文本。
	- 🌀🌌▦ **[向量空間](04-07-vector_space.zh-hant)**：神經網路無法直接處理字母或圖像等非數值資料，必須先將這些資料轉換為**向量**（`Vector`），也就是一組數字陣列，才能在由**線性代數**所定義的抽象**數學空間**中進行運算與學習。這由向量組成的抽象數學空間叫**向量空間**。
	    
- 回憶**符號流 AI**（Symbolic AI）所有條目，及其彼此關係，進而和**統計流 AI**進行參照比較。
- 思考 [第伍章 ☸](05----ai_orientations.zh-hant) AI 5 大導向（AI Orientations）的各種系統／設計思維視角，是如何運用 **符號流 AI** 或 **統計流 AI**為基礎，才能構成 **知識組織方式** 與 **問題解決策略**。