---
title: 🌀🪢 神經網路
tags:
- 激勵函數
- 深度學習
- 反向傳播
- 梯度下降
- 模式識別
- 圖像辨識
- 神經元
---
`神經網路`（Neural Networks）或譯`類神經網路`，是種受到人腦神經結構啟發的數學模型。它由多個**節點**（或稱神經元）層層相連，形成一個錯綜複雜的網路。每個節點會接收前一層的訊號，並透過一個**激勵函數**（例如 S 型函數）來決定是否將訊號傳遞給下一層，開展**剌激與反應**（Stimulus-Reaction）關係（行為主義心理學的基礎概念）的模擬。

此架構擁有強大**學習能力**。類神經網路可先利用**反向傳播**（Backpropagation）演算法計算預測誤差（即結果與實際結果的差距）；接著，利用 **[梯度下降](09-02-steepest_descent_method.zh-hant)**（Gradient Descent）等最佳化方法，逐步調整各節點的**權重**以最小化誤差。透過不斷迭代，模型最終能從輸入數據中識別出有意義的模式。

**受到**人腦**神經可塑性**（Neuroplasticity）的啟發，**新認知機**（Neocognitron）開創性地發展出分層架構，而現代的**卷積神經網路**（CNN）則繼承並強化了這些核心概念，成為當今最成功的神經網路類型之一。

## 🛅 闡明範例：圖像辨識

類神經網路最著名的應用之一是**圖像辨識**。看似簡單其實極具挑戰，因為機器只看得到像素點的數值矩陣，而非人類能辨識的形狀與紋理，類神經網路正能彌補此差距。

以辨識手寫數字為例，一個類神經網路通常包含以下結構：

- 📥 **輸入層**（Input Layer）：由數百個節點組成，每個節點代表圖像中的一個像素。圖片的像素亮度值被送入對應節點。
    
- 🧠 **隱藏層**（Hidden Layers）：由多層節點構成，是進行「思考」的核心。淺層節點可能學習辨識直線或曲線等基本特徵；隨著資訊傳遞至更深層，節點會將這些基本特徵組合成更複雜的模式，例如「圓圈」或「橫槓」。
    
- 🔢 **輸出層**（Output Layer）：通常由數個節點組成，每個節點代表一個可能的分類結果。在手寫數字辨識中，可能有10個節點分別代表數字0到9。網路會根據隱藏層的處理結果，給出每個數字的機率，並最終選擇機率最高的作為辨識結果。
    
在整個過程中，網路會不斷透過**反向傳播**和**梯度下降**調整各層之間的**權重**，從大量的訓練圖片中學習，精準地將像素模式與正確的數字標籤連結起來。

## 🎋 類型

當代神經網路可依其**學習方式**與**資料處理特性**分為以下幾種典型架構：

- ▦ **多層感知器**（MLP）：
    
    - 最基本的前饋神經網路，資訊僅沿單一方向流動，無迴圈結構。
        
    - 適用於處理靜態、非序列性的表格或結構化數據，執行分類與回歸任務。
        
    - 難以捕捉時間序列或空間關聯性，不適合處理語音或影像等資料。
        
- 🧠 **卷積神經網路**（CNN）：
    
    - 受到人類視覺皮層啟發，透過⊛卷積層與⊗池化層自動提取局部空間特徵。
        
    - 在圖像辨識、物件偵測與影片分析等任務中表現卓越，能識別形狀、邊緣與紋理。
        
    - 權重共享與局部感受野設計有效降低參數量，提升訓練效率與泛化能力2。
        
- 🔄 **遞歸神經網路**（RNN）：
    
    - 專為處理序列資料而設計，透過🔄遞歸連結與隱藏狀態⟳保留時間上下文。
        
    - 適用於語音辨識、自然語言處理與時間序列預測等任務。
        
    - 易受梯度消失或爆炸影響，在處理長序列時表現不穩定3。
        
- 🧬 **長短期記憶網路**（LSTM）：
    
    - RNN 的改良版本，引入🔒門控機制（輸入門、遺忘門、輸出門）以控制資訊流。
        
    - 能選擇性記憶或遺忘資訊，有效捕捉長期依賴，克服梯度消失問題。
        
    - 廣泛應用於機器翻譯、語音生成與複雜文本分析等序列任務3。
        
- 🧞‍♂️ **變換器**（Transformer）：
    
    - 完全捨棄遞歸結構，改用多頭自注意力機制來捕捉序列中的遠程依賴。
        
    - 可並行處理整個序列，大幅提升訓練效率與模型表現。
        
    - 為大型語言模型（LLM）如 GPT、BERT 等的核心架構，革新自然語言處理領域。


## 🌟 數學基礎與優勢

類神經網路的核心數學基礎是**線性代數**（用於高效的矩陣運算）和**微積分**（用於最佳化學習過程）。正是這些數學工具，讓網路能夠有效處理大量數據並解決複雜問題。

它的主要優勢在於：

- **🔀 高度彈性與可擴展性**：網路的層數和節點數可以根據問題的複雜度自由調整。增加層次後，便形成了能夠處理更抽象數據模式的**深度學習**模型。
    
- **🧠 自動學習特徵**：傳統機器學習方法通常需要工程師手動設計特徵，但類神經網路能直接從原始數據（例如監控影像的像素點）中自動辨識並提取有用的模式，這讓模型得以處理更廣泛、更複雜的數據。
    
- **📈 強大的非線性建模能力**：透過**激勵函數**與**[梯度下降](09-02-steepest_descent_method.zh-hant)**，網路能夠處理任何複雜的非線性關係，解決單純線性模型無法應對的現實問題。簡單來說：
	- - **激勵函數**（Activation Function）：神經元的啟用開關，是**神經元**內部的「決策者」。激勵函數的作用是決定一個神經元是否應該被「激發」（activate），以及被激發的程度。它接收來自前一層神經元的加權輸入總和，然後將其轉換為輸出訊號。
		- **功能**：引入非線性，讓神經網路能夠學習和處理更複雜的模式。如果沒有激勵函數，無論網路有多少層，它都只能處理線性問題，就像是一個簡單的單層神經網路。
		- **範例**：像是 `Sigmoid`、`ReLU` 或 `Tanh` 函數。
	- **梯度下降**（Gradient Descent）：神經網路的學習演算法，是**整個網路**的「學習者」。梯度下降是一種**最佳化演算法**。它的目標是最小化神經網路的預測誤差，也就是讓網路的輸出結果盡可能接近真實答案。
		- **功能**：它透過計算誤差相對於網路中所有**權重**的「梯度」（可以理解為誤差下降最快的方向），然後沿著這個方向小幅調整權重，不斷重複這個過程，直到誤差降到最低。
		- **用途**：梯度下降是訓練神經網路的核心步驟。它幫助網路從數據中學習，讓網路變得越來越準確。
    
總之，神經網路利用其數學結構模擬人腦的

激勵函數是神經網路架構的一部分，而梯度下降是訓練這個架構的方法。神經網路需要先有激勵函數，才能使用梯度下降進行學習。

儘管如此，神經網路的局限性也很明顯。它需要龐大的數據與**GPU 算力**進行訓練，且由於其複雜的內部結構，通常被視為一個 **「黑箱」**。例如，當一個神經網路正確辨識出「威脅」或「異常」時，我們很難確切解釋它究竟是基於哪些像素、形狀或紋理組合來做出這個判斷，這使得它在需要高透明度的應用場景中面臨挑戰。

## 🧠🕸️🔮 神經網路、深度學習與貝氏網路的關係

神經網路、深度學習和貝氏網路是 AI 模型中的三種關鍵思維，各自有不同的核心功能：

- **神經網路**：其核心是透過模仿人腦神經元的結構，自動從數據中學習並識別複雜的**模式**。
    
- **深度學習**：這是一種特別強大的神經網路，因擁有許多隱藏層而能從海量數據中提取更抽象、更深層次的特徵。它的成功，主要得益於**GPU 算力**的普及，使複雜模型的訓練成為可能。
    
- **貝氏網路**：其核心是**機率與因果推論**。它擅長處理不確定性問題，並提供每個判斷的機率依據。結合兩者優點的**貝氏深度學習**也應運而生，它為傳統深度學習提供了一種解決「黑箱」和「不確定性」問題的優雅方法。
    

簡而言之，**神經網路擅長「模式識別」**，而**貝氏網路擅長「因果推論」**。這兩種模型各有專精，並非相互取代。近年來，新興的**貝氏深度學習**領域，正是結合兩者優勢的嘗試，讓強大的深度學習模型也能像貝氏網路一樣，提供判斷的**信心水準**，使其應用在醫療診斷等高風險領域時更為可靠。

***

## ✨ 小結

類神經網路是現代 AI 最核心的技術之一。它從模仿人腦的簡單概念出發，透過強大的數學與演算法，發展成能自動學習、處理複雜數據的強大工具。它不僅推動了圖像、語音、自然語言處理等領域的革命，也為我們探索更深層次的 AI 提供了可能。


***

## 👉 接下來

- 回憶**統計流 AI**（Statistical AI）其它條目，評估自己可不可以說明**類神經網路**和他們的關係，如下所述：
	- **🌀🎲 機率性關聯**：神經網路的學習過程，本質上就是在尋找輸入數據與輸出結果之間的**機率性關聯**。例如，它會學會一個像素組合（特徵）與特定數字（結果）之間有很高的關聯性。
	    
	- **🌀📦 機器學習模型**：神經網路是一種強大的**機器學習模型**實例（instance）。
	    
	- **🌀🛠️ 特徵工程**：神經網路的一大優勢是它能**自動學習特徵**，這大大減少了傳統機器學習中對**特徵工程**的依賴。在過去，人們必須手動設計要給模型的數據特徵，但神經網路能從原始數據中自己找到這些特徵。
	    
	- **🌀😵‍💫 大語言模型**：**大語言模型**（LLM）本身是一種強大的**神經網路**實例（instance）。它們擁有龐大的層次和參數，因此能夠理解和生成複雜的文本。
	    
	- 🌀🌌 **向量空間**：神經網路無法直接處理字母或圖像等非數值資料，必須先將這些資料轉換為**向量**（`Vector`），也就是一組數字陣列，才能在由**線性代數**所定義的抽象**數學空間**中進行運算與學習。這由向量組成的抽象數學空間叫**向量空間**。
	    
- 回憶**符號流 AI**（Symbolic AI）所有條目，及其彼此關係，進而和**統計流 AI**進行參照比較。
- 思考 AI 5 大導向（AI Orientations），見[第伍章 ☸](05----ai_orientations.zh-hant)的各種系統／設計思維視角，是如何運用 **符號流 AI** 或 **統計流 AI**為基礎，才能構成 **知識組織方式** 與 **問題解決策略**。