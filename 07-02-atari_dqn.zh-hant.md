---
tags:
- DeepMind
- 深度強化學習
- Q-Learning
- Atari
- 遊戲AI
- 深度學習
- 神經網路
- 強化學習
- 非對抗型博弈
- 對抗型博弈
---
# 🕹️👾 Atari DQN🏆 {#sec-atari-dqn}

`Atari DQN`（Atari Deep Q-Network）是由 Google DeepMind 於 2013 年開發的一種人工智慧模型 Deep Q-Network（DQN），能在數十款經典 Atari 2600 遊戲中，僅以螢幕像素作為輸入，就能達到甚至超越人類玩家的表現。

DQN 的成功，標誌著**深度學習**與**強化學習**的首次完美融合，開創了「**深度強化學習**」（Deep Reinforcement Learning）這個重要的研究領域。

## 🤼 核心博弈類型：非對抗型 vs. 對抗型

DQN 的重大突破在於，它能以同一套模型架構，成功處理兩種截然不同的遊戲類型，這與**電子老鼠**專注於**非對抗型**任務有所不同。

- 🧘 **非對抗型博弈（Non-Adversarial Games）**
    
    - **特徵**：AI 無需與直接對手互動，其目標是單純地在環境中追求最佳表現，例如獲得最高分或生存最久。
        
    - **🕹️ Atari 案例**：在《打磚塊》（Breakout）中，AI 透過控制球拍來擊破磚塊，其挑戰來自於**遊戲物理機制**與**自身表現的優化**。AI 只需要學習如何最大化分數，而沒有來自對手的策略干擾。
        
- ⚔️ **對抗型博弈（Adversarial Games）**
    
    - **特徵**：AI 必須直接與遊戲內建的**虛擬對手**進行策略對抗，其目標是透過出色的策略來「擊敗」對手。
        
    - **👾 Atari 案例**：在《太空侵略者》（Space Invaders）中，AI 必須預測敵人的移動並進行反擊；在《乓》（Pong）中，AI 則要與另一方球拍進行互動，攻防往來。這些都屬於**對抗型博弈**，考驗 AI 在有對手環境下的決策能力。

## 🕹️👾 博弈挑戰與歷史

在 DQN 誕生前，傳統 AI 難以在 Atari 遊戲中取得成功，因為這類遊戲的挑戰來自於高維度的感官輸入和延遲的獎勵機制。

### 🎮 關於 Atari 2600

Atari 2600 是一款在 1970 至 80 年代風靡一時的家用遊戲主機。儘管其遊戲畫面像素低、操作簡單（通常只有一個搖桿和一個按鈕），但遊戲內容卻需要玩家具備靈敏的反應與策略規劃能力。遊戲的狀態並非像西洋棋那樣有明確的規則可循，AI 必須從不斷變化的螢幕畫面中學習。

### 🏆 博弈挑戰

DQN 團隊發現，若想開發能玩這類遊戲的 AI，必須克服以下幾個技術挑戰：

- 🖼️ **高維度感知輸入**：傳統 AI 通常處理結構化的數據（如棋盤上的座標），但 Atari 遊戲的輸入是原始的**像素**畫面，這是一個巨大的數據矩陣。AI 必須能從這些像素中，理解遊戲的物件（如玩家、敵人、道具）和情境。
    
- ⏳ **延遲獎勵**：AI 的行動（如向右移動）通常不會立即獲得獎勵（如分數）。一個成功的操作可能需要數秒甚至數分鐘後才會產生效果。AI 必須學會將當前的行動與未來的獎勵關聯起來。
    
- 🌌 **巨大的狀態空間**：即使是簡單的 Atari 遊戲，所有可能的螢幕畫面組合也多得難以計算，這使得傳統的「查表法」學習策略完全行不通。
    

### 📜 博弈歷史

- 📄 **2013年**：Google DeepMind 在《自然》（Nature）期刊上發表了 DQN 相關論文，展示了一個 AI 模型能在《乓》（Pong）、《打磚塊》（Breakout）和《太空入侵者》（Space Invaders）等遊戲上表現出色。這是首個能直接從像素輸入，學會玩遊戲的 AI 模型。
    
- 🏅 **2015年**：DeepMind 發表了改進版的 DQN，能以超人的水平玩超過 49 款 Atari 遊戲。它的成功證明了深度強化學習的通用性，即一個模型可以無需修改，在多個不同任務上進行學習。
    

## ✅ 克服難點方式

DQN 能夠克服這些挑戰，核心在於巧妙地結合了**深度神經網路**和傳統的**Q-學習**演算法，並引入了**經驗回放**機制。

- 🧠 **Q-Learning (Q-學習)**：這是一種強化學習演算法，它會讓 AI 學習在特定**狀態**（State）下，採取某個**行動**（Action）能獲得多少**獎勵**（Reward），這個獎勵值被稱為 **Q-值**。傳統的 Q-學習透過一張巨大的表格來記錄每個狀態與行動的 Q-值，但這在 Atari 這種狀態空間巨大的遊戲中無法實現。
    
- 💻 **深度神經網路**（Deep Neural Network）：DQN 使用**深度神經網路**來取代那張表格。這個神經網路的輸入是遊戲畫面，輸出則是每個可能行動的 Q-值。這樣，AI 就不需要儲存每一個可能的畫面，而是學會如何從任何畫面中**「估計」**出最佳的行動。
    
- 🔄 **經驗回放**（Experience Replay）：為了讓學習更穩定，DQN 引入了**經驗回放**的機制。它會把每次與遊戲的互動（狀態、行動、獎勵、新狀態）都儲存在一個**記憶庫**中。在訓練時，AI 會從記憶庫中**隨機抽取**一小批經驗來進行學習。這避免了直接從連續的遊戲體驗中學習所產生的數據關聯性，有效提高了學習的效率和穩定性。
    

DQN 透過這三種機制，讓 AI 能夠從原始像素中理解遊戲規則，並學會如何在延遲獎勵的環境中做出長期有效的決策。

***

## 💡 AI 應用啟發

Atari DQN 的案例，展示了深度強化學習在解決複雜決策問題上的巨大潛力。

- **🎯 問題意識**：適用於**需要從高維度感官輸入中學習、且獎勵延遲**的任務，例如機器人學、自動駕駛、或需要從大量即時數據中做出最佳決策的自動化系統。
    
- **🗺️ 建構資源**：DQN 的成功顯示，要建構這類 AI，關鍵資源不是大量標註過的數據，而是能夠與之互動的**環境**和一個清晰的**獎勵機制**。
    
- **⚡ 智能加值**：DQN 證明了一個單一的 AI 模型，能透過學習在多個完全不同的任務上都表現出色，這體現了**通用學習**的能力，而不是針對單一任務的特定程式。
    
- **🏭 佈署條件**：適合在**可以被簡化為「感知－決策－行動」循環**的任務中部署，且環境變動可透過感測器有效捕捉。
    

***

## 👉 下一部分

在理解**Atari DQN**如何將 AI 帶入深度強化學習的時代後，同時克服**非對抗型博弈**與**對抗型博弈**賽局後，接下來將探討 **[AlphaGo 圍棋](07-03-alphago.zh-hant.md)**（AlphaGo），看它如何將 AI 推向另一高峰，在圍棋這類**完全資訊博弈**中展現超越人類的策略思考能力。
