---
tags:
 - 完全資訊博弈
 - 神經網路
 - 蒙地卡羅樹搜尋
 - Google
 - DeepMind
---
# ⚫⚪ AlphaGo🏆 {#sec-alphago}

`AlphaGo 圍棋`或譯 `阿爾法圍棋`（AlphaGo）標誌著人工智慧（AI）在博弈挑戰上的重要里程碑。由 **DeepMind** 公司採用其 **[深度學習](04-03-neural_networks)** 為核心技術，於2016年擊敗韓國職業棋手 **李世乭**，成為勝過人類精英棋手的博弈派 AI 經典案例。

## 🏆 博弈挑戰

圍棋歷史悠久且複雜，在東亞文化圈有數千年的歷史，長期以來是人工智慧難以征服的堡壘。因候選棋步遠多於西洋棋等（約$10^{360}$次方），圍棋算是在**完全資訊博弈**問題領域中最具挑戰性的賽局。

雖然僅分成白和黑棋，圍棋的具體挑戰主要體現在兩個方面：

- **評估函數**（evaluation function）難生成：由於候選棋步極多，計算當前局面的全盤勝率極為困難。
- **交互動態** 變動大：盤面狀態高度動態，一步棋常常會對整體格局產生巨大的影響，難以進行穩定的長期規劃。

鑑於這些挑戰，在 AlphaGo 圍棋出現之前，AI 主要依賴於預設的局面知識和有限的搜索來撰寫圍棋評估程式，效果有限。

## 📜 博弈歷史

AlphaGo 的成功是 DeepMind 公司長期專注於類神經網路研究的成果。AlphaGo 圍棋在 2016 挑戰成功前，有以下的發展歷史：

* **2010年**：**Demis Hassabis**（戴米斯·哈薩比斯）、**Shane Legg**（尚恩·利格）和 **Mustafa Suleyman**（穆斯塔法·蘇萊曼）於 倫敦創建 **DeepMind** 公司，特別採用 **類神經網路** 為核心技術。
* **2014年**：**Google** 併購 DeepMind，重新命名為 **Google DeepMind**，突顯其 **深度學習** 研究實力。
* **2015年**：**AlphaGo** 開發出 **第一代**，在 **歐洲圍棋大賽** 擊敗歐洲冠軍樊麾（Fan Hui）。
* **2016年**：**AlphaGo** 以 **4:1** 擊敗世界圍棋冠軍 **李世乭**，正式確立了 AI 在圍棋領域的領先地位。

這次歷史性的勝利，標誌著 AI 第一次在完全資訊博弈中戰勝人類精英。AlphaGo 圍棋的核心方法是透過模擬對手隨機落子反覆計算勝率，以找到最佳決策。

## ✅ 克服難點方式

**AlphaGo 的核心突破在於，它有效地將深度學習的模式識別能力與傳統搜索算法的規劃能力結合起來。** DeepMind AlphaGo 實踐了**可評估棋盤局面的直覺思考**。它透過結合 **[蒙地卡羅樹搜尋](09-06-monte_carlo_tree_search)** 及 [深度學習](04-03-neural_networks) 的方式，分以下兩個階段克服了圍棋的複雜性：

1.  **大數據學習**（監督式學習）：從龐大的棋譜資料庫中學習人類專家的下棋模式，如同職業棋士去調較策略函數（policy function），並用 **[蒙地卡羅樹搜尋](09-06-monte_carlo_tree_search)** 實現。
2.  **自我博弈強化學習**：在此階段，AlphaGo 與自身對弈來產生新的經驗數據，利用 **[神經網路](04-03-neural_networks)** 建模學習，從而引導 AI 選擇勝率高的下一步，達到超越人類策略的境界。

透過這兩個階段的學習，AlphaGo 成功地在巨大的搜索空間中進行高效決策，並獲得了人類難以企及的「直覺」判斷能力。

## 🔑 關鍵技術

AlphaGo 成功的基石是深度強化學習（Deep Reinforcement Learning, DRL）的創新應用，巧妙地將神經網路集成到搜索過程中。

DeepMind AlphaGo 具體技術實現 **深度強化學習** 包含：

* **卷積神經網絡**（CNNs）：用於將棋盤狀態（高維輸入）進行特徵提取，以便於模型理解當前局面。
* **策略網絡**（Policy Network）：是一個深度卷積網絡，用於估計在當前狀態下，每一步走法的機率（$P(a|s)$），從而大幅度縮減搜索的廣度。
* **價值網絡**（Value Network）：是另一個深度卷積網絡，用於估計當前局面下最終的勝率（$V(s)$），從而指導搜索的深度。
* **[蒙地卡羅樹搜尋](09-06-monte_carlo_tree_search)**（MCTS）：以模擬為基礎，它利用策略網絡和價值網絡引導搜索，實現高效的樹搜索，而非傳統 AI 的蠻力窮舉。

這些技術的結合讓 AlphaGo 實現了高效率的搜索和精準的局面評估。

## 💡 AI 應用啟發

AlphaGo 的案例為未來 AI 的發展和應用提供了一個強有力的藍圖，特別是在決策與規劃領域，為 AI 應用帶來以下啟發：

- **🎯 問題意識**：證明了 AI 尤其適用於 **有明確目標與規則的決策** 任務。
- **🗺️ 建構資源**：表明了全局或全面格局的「世界框架」**能被數字化表示**來訓練模型，這是應用 DRL 的先決條件。
- **⚡ 智能加值**：AI 能在**海量數據**中學到超越人類專家知識的策略，並透過**自我博弈**強化學習成果，實現智能的持續增長。
- **🏭 佈署條件**：這類 AI 適合在**可控、半結構化或全結構化**的作業環境中部署，並可依需求擴展到物理空間的任務規劃。

DeepMind AlphaGo 展示了機器在**完全資訊博弈**問題領域中，能通過深度學習超越人類精英。

### 對「博弈派」AI 的核心貢獻

AlphaGo 是博弈派 AI 的里程碑，其技術架構和學習範式對後續的博弈 AI 產生了深遠影響。

* 首次成功結合 **蒙地卡羅樹搜尋** 與 **深度神經網路** 進行決策，為後來的博弈 AI 奠定基礎。
* 證明了 **深度強化學習** 在處理極高維度和巨大搜索空間問題上的強大能力。
* 確立了**自我對弈**作為在沒有人類頂級數據情況下產生超人策略的有效方法（儘管第一代 AlphaGo 仍依賴人類棋譜，但後續版本如 AlphaGo Zero 則完全採用自我對弈）。

### 對「具身派」AI 的啟發

儘管圍棋是純粹的模擬遊戲，AlphaGo 的成功也為需要在物理世界中進行決策的具身智能提供了重要思路。

* 儘管圍棋是模擬遊戲，但其決策機制（從狀態到行動的規劃）啟發了**任務與目標規劃**的實體應用。
* 強調了 AI 需要一個內在的**價值評估機制**（如價值網絡）來指導行動，這與具身智能在複雜物理世界中進行目標導向行動的需求相似。

***

## 👉 下一部分

在理解 **[神經網路](04-03-neural_networks)** 深度學習 與 **[蒙地卡羅樹搜尋](09-06-monte_carlo_tree_search)** 數學工具 助 AlphaGo 圍棋打敗人類精英棋手后，接下來探討 [撲克 AI](07-04-poker_ai.zh-hant.md)（Libratus / Pluribus）在**不完全資訊博弈**中的挑戰及克服方法啟發。
