---
title: 🏆⚫⚪  AlphaGo
tags:
  - 完全資訊博弈
  - 神經網路
  - 蒙地卡羅樹搜尋
  - Google
---

`AlphaGo 圍棋`或譯 `阿爾法圍棋`（AlphaGo）標誌著人工智慧（AI）在博弈挑戰上的重要里程碑。由 **DeepMind** 公司採用其 **[深度學習](04-03-neural_networks)** 為核心技術，於2016年擊敗韓國職業棋手 **李世乭**，成為勝過人類精英棋手的博弈派 AI 經典案例。

## ⚪⚫ 博弈挑戰與歷史

圍棋歷史悠久，在東亞文化圈有數千年的歷史。因候選棋步遠多於西洋棋等（約10的360次方），圍棋算是在**完全資訊博弈**問題領域中最具挑戰的賽局。

### 🏆博弈挑戰

雖然僅分成白和黑棋，圍棋的具體挑戰有：

- **評估函數**（evaluation function）難生成－－計算勝率時，因候選棋步多，全盤評估難；
- **交互動態** 變動大－－常常一步棋，影響格局大；

因此，在 AlphaGo 圍棋之前，主要基於各種不同局面來撰寫AI 圍棋評估程式。

### 📜博弈歷史

AlphaGo 圍棋在 2016 挑戰成功前，有以下的發展歷史：

* 2010年，**Demis Hassabis**（戴米斯·哈薩比斯）、**Shane Legg**（尚恩·利格）和 **Mustafa Suleyman**（穆斯塔法·蘇萊曼）於 **倫敦** 創建 **DeepMind** 公司，特別採用 **類神經網路** 為核心技術；
* 2014年，**Google** 併購 DeepMind，重新命名為 **Google DeepMind**，突顯其 **深度學習**研究實力；
* 2015年，**AlphaGo** 開發出 **第一代**，在 **歐洲圍棋大賽** 擊敗歐洲冠軍樊麾（Fan Hui）；
* 2016年，**AlphaGo** 以 **4:1** 擊敗世界圍棋冠軍 **李世乭**。

如下段所詳述，DeepMind 採用 [深度學習](04-03-neural_networks)，並以模擬為基礎實踐 **[蒙地卡羅樹搜尋](09-06-monte_carlo_tree_search)**，以模擬對手隨機落子反覆計算勝率。

### ✅克服難點方式

DeepMind AlphaGo 以結合 **[蒙地卡羅樹搜尋](09-06-monte_carlo_tree_search)** 及深度學習的方式，分以下2學習階段：

1. **大數據學習**－－從龐大的棋譜資料庫，如同職業棋士去調較策略函數（policy function），並用 **[蒙地卡羅樹搜尋](09-06-monte_carlo_tree_search)** 實現；
2. 自我博弈**強化學習**－－利用 **[神經網路](04-03-neural_networks)** 建模學習，從而引導 AI 選擇勝率高的下一步。

可以說，經神經網路深度學習，AlphaGo 實踐可評估棋盤局面的直覺思考。

## 💡 AI 應用啟發

DeepMind AlphaGo 展示了機器在人機博弈時，預先利用神經網路深度學習人類**已有的相關知識的總和**，是能在像圍棋一樣的**完全資訊博弈**問題領域，戰勝人類精英的。

此案例對 AI 應用有以下啟發：

- **🎯 問題意識**：適用於 **有明確目標與規則的決策** 任務。
    
- **🗺️ 建構資源**：全局或全面格局的世界框架**能被數字化表示**來訓練模型。  
    
- **⚡ 智能加值**：能在**海量數據**中學到超越人類專家知識的策略，並**自我博弈**強化學習成果。
    
- **🏭 佈署條件**：適合在**可控、半結構化或全結構化**的作業環境中部署，並可依需求擴展到物理空間。

***

## 👉 下一部分

在 理解 **[神經網路](04-03-neural_networks)** 深度學習 與  [蒙地卡羅樹搜尋](09-06-monte_carlo_tree_search) 數學工具 助 AlphaGo 圍棋打敗人類精英棋手后，接下來探討 [撲克 AI](07-04-poker_ai.zh-hant.md)（Libratus / Pluribus）的博弈挑戰及克服方法啟發。








