---
tags:
  - 完全資訊博弈
  - 神經網路
  - 蒙地卡羅樹搜尋
  - Google
  - DeepMind
---
# ⚫⚪  AlphaGo🏆 {#sec-alphago}

`AlphaGo 圍棋`或譯 `阿爾法圍棋`（AlphaGo）標誌著人工智慧（AI）在博弈挑戰上的重要里程碑。由 **DeepMind** 公司採用其 **[深度學習](04-03-neural_networks)** 為核心技術，於2016年擊敗韓國職業棋手 **李世乭**，成為勝過人類精英棋手的博弈派 AI 經典案例。

## 🏆博弈挑戰

圍棋歷史悠久，在東亞文化圈有數千年的歷史。因候選棋步遠多於西洋棋等（約10的360次方），圍棋算是在**完全資訊博弈**問題領域中最具挑戰的賽局。

雖然僅分成白和黑棋，圍棋的具體挑戰有：

- **評估函數**（evaluation function）難生成－－計算勝率時，因候選棋步多，全盤評估難；
- **交互動態** 變動大－－常常一步棋，影響格局大；

因此，在 AlphaGo 圍棋之前，主要基於各種不同局面來撰寫AI 圍棋評估程式。

## 📜博弈歷史

AlphaGo 圍棋在 2016 挑戰成功前，有以下的發展歷史：

* 2010年，**Demis Hassabis**（戴米斯·哈薩比斯）、**Shane Legg**（尚恩·利格）和 **Mustafa Suleyman**（穆斯塔法·蘇萊曼）於 倫敦創建 **DeepMind** 公司，特別採用 **類神經網路** 為核心技術；
* 2014年，**Google** 併購 DeepMind，重新命名為 **Google DeepMind**，突顯其 **深度學習**研究實力；
* 2015年，**AlphaGo** 開發出 **第一代**，在 **歐洲圍棋大賽** 擊敗歐洲冠軍樊麾（Fan Hui）；
* 2016年，**AlphaGo** 以 **4:1** 擊敗世界圍棋冠軍 **李世乭**。

AlphaGo 圍棋以模擬對手隨機落子反覆計算勝率。

## ✅克服難點方式

DeepMind AlphaGo 實踐**可評估棋盤局面的直覺思考**。它透過結合 **[蒙地卡羅樹搜尋](09-06-monte_carlo_tree_search)** 及[深度學習](04-03-neural_networks)的方式，分以下兩個階段克服了圍棋的複雜性：

1. **大數據學習**（監督式學習）：從龐大的棋譜資料庫學習，如同職業棋士去調較策略函數（policy function），並用 **[蒙地卡羅樹搜尋](09-06-monte_carlo_tree_search)** 實現。 
2. **自我博弈**強化學習：利用 **[神經網路](04-03-neural_networks)** 建模學習，從而引導 AI 選擇勝率高的下一步。

## 🔑 關鍵技術 
DeepMind AlphaGo 的實現核心是 **深度強化學習** 的成功應用，具體技術包含： 

* **卷積神經網絡**（CNNs）：用於棋盤狀態）的特徵提取。 
* **策略網絡**（Policy Network）：學習最佳的下一步走法（$P(a|s)$）。 * **價值網絡**（Value Network）：估計當前局面下最終的勝率（$V(s)$）。 
* **[蒙地卡羅樹搜尋](09-06-monte_carlo_tree_search)**（MCTS）：以模擬為基礎，利用策略網絡和價值網絡引導搜尋，而非蠻力窮舉。


## 💡 AI 應用啟發

DeepMind AlphaGo 展示了機器在**完全資訊博弈**問題領域中，能通過深度學習超越人類精英，為 AI 應用帶來以下啟發：

- **🎯 問題意識**：適用於 **有明確目標與規則的決策** 任務。
    
- **🗺️ 建構資源**：全局或全面格局的世界框架**能被數字化表示**來訓練模型。  
    
- **⚡ 智能加值**：能在**海量數據**中學到超越人類專家知識的策略，並**自我博弈**強化學習成果。
    
- **🏭 佈署條件**：適合在**可控、半結構化或全結構化**的作業環境中部署，並可依需求擴展到物理空間。

DeepMind AlphaGo 展示了機器在人機博弈時，預先利用神經網路深度學習人類**已有的相關知識的總和**，是能在像圍棋一樣的**完全資訊博弈**問題領域，戰勝人類精英的。

### 對「博弈派」AI 
* 首次成功結合 **蒙地卡羅樹搜尋** 與 **深度神經網路** 進行決策，為後來的博弈 AI 奠定基礎。 
* 證明了 **深度強化學習**  在處理極高維度和巨大搜索空間問題上的強大能力。 * 確立了**自我對弈**作為在沒有人類頂級數據情況下產生超人策略的有效方法（儘管第一代 AlphaGo 仍依賴人類棋譜）。

### 對「具身派」AI 

* 儘管圍棋是模擬遊戲，但其決策機制啟發了**任務與目標規劃**的實體應用。 
* 強調了 AI 需要一個內在的**價值評估機制**（如價值網絡）來指導行動，這與具身智能在複雜物理世界中進行目標導向行動的需求相似。

***

## 👉 下一部分

在 理解 **[神經網路](04-03-neural_networks)** 深度學習 與  [蒙地卡羅樹搜尋](09-06-monte_carlo_tree_search) 數學工具 助 AlphaGo 圍棋打敗人類精英棋手后，接下來探討 [撲克 AI](07-04-poker_ai.zh-hant.md)（Libratus / Pluribus）的博弈挑戰及克服方法啟發。






