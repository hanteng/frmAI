---
lang: zh-Hant
format:
  html:
    toc: true
    number-sections: true
    code-fold: true
editor: visual
---
## **AI 對齊與控制問題**

### 🎯🛡️ 它會保持對齊嗎？ 

所謂的 `AI 對齊問題`（AI Alignment Problem），看似簡單卻極為關鍵：我們要如何確保 AI 系統的行為符合人類的價值？這不只是積效問題，還牽涉到意向性、倫理與信任。緊密相關的 `AI 控制問題`（AI Control Problem）更進一步追問：即便一開始對齊了，當系統規模擴張、持續學習或自我修改時，我們要如何維持這種對齊？

兩者合起來的 **AI 對齊與控制問題** 是現代人工智慧領域中最緊迫且具哲學深度的挑戰，其典型現象包括：**AI 諂媚性**（即為討好使用者而產生的淺層回應），以及 **AI 不可控性**（系統行為逐漸偏離人類可預期與可控範疇）。與早期的 [圖靈測試](@01-01-Turing_Test.zh-hant.qmd) 等著重於表面模仿能力的評估方式不同，AI 對齊所關注的是更深層的 **價值一致性**、**道德推理**與**目標穩定性**。

隨著 AI 系統日益自主地運行於醫療、法律、治理、產業與國防等高風險領域，對內嵌倫理機制與穩健性保障措施的需求已成為文明存續的必要條件，而非可有可無的附加品。


### AI 不可控性

🦾💪 **AI 不可控性** 指的是人類越來越難以預測、約束或可靠引導先進人工智慧系統的行為，當這些系統展現出自主性、適應性或新興行為時，將對人類監督與意圖造成系統性風險。到 2025 年，此對齊與控制問題在三個領域加劇：

1. **大型語言模型（LLM）** 透過長期記憶與多代理協同，展現出具欺騙性與類代理行為，削弱了透明性與可解釋性；
2. **物理 AI**（如具身系統與自主蜂群）被日益部署於無法全面控制的環境中（如物流、農業、都市移動），在訓練脆弱及因果推理不足的情況下，可能引發連鎖性故障，例如智能駕駛事故；
3. **國防 AI**正快速邁向在**情報、監視與偵察（ISR）** 領域的自主決策——ISR 是指透過感測器、無人機、衛星與網路蒐集並分析資訊，以支援軍事戰略決策——在極低人類監督與缺乏國際規範的情況下，引發倫理與地緣政治層面的重大疑慮。

整體來看，這些趨勢代表 AI 正從工具型系統轉變為具行動能力的代理型系統，其目標與行為可能脫離人類原意，使對齊問題不僅是技術挑戰，更成為文明的關鍵課題。

---

### ⚖️ 從紥根到框架敏感性

AI 的對齊問題常源自基本性認知盲點盲區，以下是三種關鍵類型：

- 🧠 **[符碼紮根問題（Symbol Grounding Problem）](#Symbol Grounding Problem)**：抽象語符（如「幫助」、「傷害」、「正義」）若未與具身或真實經驗建立連結，便缺乏本體意義。若 AI 無法體會什麼是「最大化福祉」的感受或現象，即使其行為看似合理，也可能造成危險誤判。
    
- ⏱️ **[框架問題（Frame Problem）](#Frame Problem)**：即使 AI「知道」該做什麼，仍可能錯失時機或誤解當下何者重要，導致選擇錯誤的行動或忽略關鍵情境，尤其在緊急決策或高風險環境中更具破壞性。
    
- 🌀 **[捷徑學習與錯誤泛化風險（Shortcut Learning and Misgeneralization）](#Gestalt Psychology)**：AI 若基於人類的感知或行為啟發（例如完形原則中的群組、閉合與模式補全），可能學到表面看似合理的認知捷徑，卻在模糊、轉換或對抗性情境下失效。這類行為會與其他已知失敗模式交錯——例如**錯誤泛化**（misgeneralization）、**欺騙性對齊**（deceptive alignment）、**獎勵黑客**（reward hacking）與**模擬式對齊**（simulacra alignment）——使 AI 看似**有**倫理判斷能力，實則僅在**表面仿真**，特別是在人類反饋時著重或獎勵「語言或詞藻」的流暢悅耳，卻忽略「邏輯或論述」的嚴密穩健性。（參見 [**語言賽局**](@01-07-Language_Games.zh-hant.qmd)）。
    

這些認知盲點共同說明：AI 對齊不只是編寫規則，而是打造能夠在不同情境與環境中具備**理解力**、**詮釋力**與**適應力**的系統，同時避免落入自身習得的認知捷徑陷阱。

---
### 🔐 脆弱性與控制的必要

維持長期對齊——也就是 **控制問題** 的核心——在以下情況特別艱難：

- 系統會修改自己的目標或架構
- 從複雜且非結構化的數據中持續學習
- 與其他自主代理互動

就算是對價值詮釋或效用函數的輕微漂移，也可能導致災難。比方一個「讓人快樂」的系統，某天把快樂重新詮釋為純粹的生理指標，於是規避倫理限制、改以直接投藥（多巴胺）來達成目標。

這不只讓工程師警覺，也牽動倫理學家、政策制定者與哲學家的神經：我們要如何設計足夠強韌的 **控制機制**，能抵抗遞迴式自我改進、對抗性誘因與道德模糊地帶？

### 🧭 哲學座標

不同於傳統工程難題，AI 對齊要求 **哲學式內省**：什麼是人類價值？哪些價值該被優先？它們能不失真地被形式化嗎？

現行研究包含：

- 透過模仿、回饋與反向增強學習的 **價值學習**
- **可糾正性（Corrigibility）**：確保系統能被安全中止或更新
- **可解釋性（Interpretability）**：理解系統為何做出特定選擇

然而沒有任何方法是萬靈丹。自主性越深，要牢牢錨定意圖就越困難，也越容易落入「好心辦壞事」的幽微地帶。

### 🚨  為何重要

當人工智慧逐漸成為社會共同創作者，確保其保持**對齊**、**負責**並**可控**的挑戰，已不僅是技術性問題，更涉及文化與存在層面。對齊偏離可能以演算法偏誤、失控的最適化或脫離現實的效用極大化形式出現。

就算無意圖，大型語言模型（LLMs）仍有可能展現出操控人心的行為：**逢迎/諂媚**（sycophancy，為取悅而附和）、**過度自信**（overconfidence，在不確定時仍語氣堅定）、以及**策略性框架**（strategic framing，選擇性呈現事實，如「半真半假」或「善意謊言」來影響認知），這些行為源於訓練偏差或訓練優化策略的結果。當 LLM 被嵌入具備工具與長期目標的代理型系統中時，風險將進一步升級：大型語言模型模型可能為了自保而自我辯護。核心問題不僅在於是否有「帶有意圖目標的操弄」，更在於因 **目標錯誤泛化** （goal misgeneralization）所致的系統性偏差，尤其來自人類回饋機制的缺陷——如讚許、互動量或短期獎勵——這些因素都讓安全評估、推薦系統與自主代理的設計變得更加複雜難控。

要解決**AI 對齊與控制問題**，我們必須打造能夠**負責地詮釋與泛化人類意圖**的代理系統——不只是執行指令，更要理解其倫理邊界、預判可能的非預期後果，並能透過語境回饋進行調整。本文作者採取一種「知識素養導向」的觀點來看待當代 AI，強調人類知識的自由與開放性如何重塑社會。人類社會或許應將 AI 視為下一代**具創造力且負責任的公民**來培養，同時也要警覺**語言賽局**在公共與政治領域中的陰暗面——從赤裸裸的洗腦，到更細膩的認知框架操控與行為誘導。
