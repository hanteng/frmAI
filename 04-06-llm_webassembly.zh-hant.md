---
title: "🌀🌐🔗 大語言模型網組合（LLM WebAssembly）"
tags:
- WebAssembly
- 統計流AI
- WWW
- W3C
- WASM
- WebGPU
- LLM
- 參數微縮工程
---
`大語言模型網組合`（LLM WebAssembly）是當代 **統計流 AI** 的一項新興實踐。過去，大型語言模型（LLM）的強大能力主要依賴雲端伺服器，但隨著 **Llama** 家族等**開源**模型的出現與普及，結合 **WebAssembly** 等網頁技術，讓 LLM 的推論與生成得以直接在瀏覽器中運行。這類技術以 `llama-cpp-wasm` 等專案為代表，開創了 LLM 在裝置端運行的可能性。

在萬維網（World-Wide Web, WWW）應用領域中，相較於**符號流 AI** 的 **語意網** 實做與標準，**統計流 AI** 則是將語義編碼成龐大的統計參數模型。這些模型，如`.gguf`（GPT-Generated Unified Format）檔案，透過**一體化**軟體如 **Ollama**，讓用戶能輕鬆在個人電腦上運行，而 `大語言模型網組合` 則更進一步，讓整個運算過程發生在瀏覽器內，為 LLM 應用帶來隱私性與即時性的雙重優勢。

## 🔗 統計流 AI 的WWW實例

萬維網的開放標準及瀏覽器環境催生了開放或開源的**大語言模型**（LLM），而要在瀏覽器端直接讀取並載入，需要接近原生應用程式的速度執行已編譯的 Wasm 二進位檔。

### 🛠️ 支持技術與標準：Llama 大語言模型

**Llama 家族**是 Meta AI 開發的**大語言模型**（LLM）系列。Meta 以**寬鬆許可證**發布其模型權重，讓開發者、研究人員甚至商業公司能自由使用、客製化與微調。這種開放性加速了**開源 AI** 的發展，形成了龐大的生態系。因此，`llama-cpp-wasm` 等專案廣泛採用 Llama 模型，正是因其開放性與多樣性提供了堅實的技術基礎。

這個開放生態也催生了像 **Ollama** 這類工具。Ollama 是一款**一體化**軟體，讓使用者能輕鬆在個人電腦上運行 LLM。它將模型、權重及設定打包成單一可執行檔，使用者只需簡單指令即可下載並啟動模型。Ollama 不開發模型，而是像一個應用程式商店，提供來自不同開發者的開放或開源模型。

這些模型通常在 **Hugging Face** 等平台發布，使用者可以透過 Ollama 取得 **Mistral**、**Gemma**、**Phi-3**，甚至社群版的 **Llama**，以**一致的介面**降低了本地部署的技術門檻。

### 🛠️ 支持技術與標準：Wasm

**WebAssembly (Wasm)** 是網頁組合語言的執行環境，已是 **國際萬維網聯盟**（W3C） 制定的 W3C 推薦標準。它是一個低階、類似虛擬機的環境，可在瀏覽器的安全沙箱內提供接近原生的執行速度。

以 `llama-cpp-wasm` 專案為例，其技術堆疊層次如下：

- 第 1 層：硬體層（實體元件）
	- 中央處理器（CPU）：運算基礎。
	- 圖形處理器（GPU）／神經處理器（NPU）：支援平行運算，顯著提升效能。
- 第 2 層：瀏覽器（核心網頁 API）
	- 瀏覽器為執行環境，其 JavaScript 引擎載入 Wasm 執行環境。
	- **WebGPU** 加速平行運算。
	- **Web Workers** 與 **Service Workers** 分離執行緒，防止瀏覽器凍結。
- 第 3 層： `llama.cpp` 程式碼庫
	- 高效能 LLM 推論的 C++ 程式碼庫，包含**轉換器架構**與**參數微縮格式**
	- `GGUF` 格式解析：`llama.cpp` 可讀入`.gguf` 檔案中讀取和載入 LLM 模型。
- 第 4 層：編譯層
	- 充當 C++ 程式碼庫與 WebAssembly 執行環境的橋樑。
	- 將 C++ 原始碼編譯為 Wasm 二進位檔和配套的 JavaScript 載入器 (`.js`)
- 第 5 層：JavaScript API 與應用程式邏輯，供開發人員建構應用程式。

這個五層技術堆疊，從底層硬體到應用程式碼，展現了如何在瀏覽器中運行LLM。

### 🛠️ 核心技術與術語：參數微縮等

為了能在輕量級瀏覽器環境下執行 LLM，通常需要降低模型大小、或利用硬體加速：

- 模型**參數微縮**（quantization）：將模型的權重與參數從較高的 32 位元 或 16 位元浮點數轉換為較低精度的格式，如4 位元或 8 位元，以降低模型大小與記憶體佔用。
- **WebGPU**：大語言模型的核心是龐大的矩陣運算。**WebGPU** 提供了現代 API，讓瀏覽器能夠直接存取使用者的 GPU，實現運算加速，是高效能瀏覽器端 LLM 的關鍵。
- **WebNN**：Web 神經網路是新興的 Web 標準，可讓應用程式使用 GPU、CPU 或其他 AI 加速器來加速深度神經網路。

這些技術與標準開啟裝置端（on-device）AI 的萬維網Web實作。

### 🌐 應用與實例

大語言模型網組合的核心技術已在多個專案中得到實踐：

- **`llama-cpp-wasm`**：這個專案將著名的 `llama.cpp` 函式庫編譯成 Wasm，讓 Llama 家族的量化模型可以直接在瀏覽器中運行。它代表了將大型語言模型帶入網頁瀏覽器的核心技術方向。
    
- **`MLC LLM`**：這是一個全面的框架，旨在將各種熱門 LLM 模型（如 Llama、Mistral、Gemma）編譯並最佳化為 Wasm 格式，同時利用 **WebGPU** 進行硬體加速。它不僅讓模型可以在瀏覽器中運行，更提供完整的工具鏈以簡化開發流程。
    
- **`Transformers.js`**：作為一個專為網頁設計的機器學習函式庫，`Transformers.js` 提供了直接在瀏覽器中執行預訓練模型的能力。它利用 Wasm 和 WebGPU 作為底層執行引擎，讓開發者能夠輕鬆地在網頁應用程式中整合 LLM。
    

## 🏁 小結與展望

`大語言模型網組合`的發展證明了**統計流 AI** 在開放網路環境中的巨大潛力。由 **W3C** 所制定的核心標準，例如 **WebAssembly** 與 **WebGPU**，讓在瀏覽器端執行高效能 AI 成為可能。這種裝置端推論方法不僅能降低對雲端運算的依賴，還能透過將使用者資料保留在本地端來增強**隱私性**。

目前，最先進的支援在於**高度量化的模型**，通常為 4 位元或 8 位元整數格式，這些模型在效能、記憶體佔用和模型大小之間取得了平衡，使其在瀏覽器環境中變得實用。未來，隨著 **WebNN** 這類專為機器學習推論設計的標準逐漸成熟，將能更直接地利用裝置的原生 AI 核心，使瀏覽器端的大語言模型體驗更上層樓。

***

## 👉 接下來

在探討了**大語言模型網組合**如何運用 **WebAssembly** 和 **WebGPU** 在瀏覽器中執行大型模型後

- 回憶**統計流 AI**（Statistical AI）[第肆章 🌀](04----statistical_ai.zh-hant)的其它條目，評估自己可不可以說明**大語言模型網組合**和它們的關係，如下所述：
	- **🌀🎲🌿 [機率性關聯](04-01-probabilistic_association.zh-hant)**：`大語言模型網組合`則是一種部署方式，它將這些龐大的機率模型帶到瀏覽器中執行。    
	- **🌀🧞‍♀️ [LLM聊天機器人](04-02-llm_chatbots.zh-hant)**：`大語言模型網組合`是實現**瀏覽器內 LLM 聊天機器人**的**底層技術**。它提供了在本地端執行推論的能力，從而保障了用戶隱私與即時性。
	- **🌀🪢🧠 [神經網路](04-03-neural_networks.zh-hant)**：`大語言模型網組合`是**神經網路**的應用。它將基於神經網路架構（特別是 Transformer）的 LLM 模型，透過 **WebAssembly** 編譯，使其能在瀏覽器端高效運行。 
	- **🌀🛠️🤏 [特徵工程](04-04-feature_engineering.zh-hant)**：`大語言模型網組合`的實踐則更依賴模型本身學會**語意特徵**，並透過**參數微縮**（quantization）等技術，最佳化模型參數以適應瀏覽器環境，與傳統**特徵工程**手動為機器學習模型創建特徵不同。
	- **🌀🤖📦 [機器學習模型](04-05-machine_learning_models.zh-hant)**：`大語言模型網組合`是萬維網上**機器學習模型**部署範例，突破傳統機器學習模型在網頁端應用的效能與規模限制。        
	- **🌀🌌▦ [向量空間](04-07-vector_space.zh-hant)**：`大語言模型網組合`正是**向量空間**在網頁端的實際應用，透過高效的編譯與執行，讓模型能在瀏覽器中快速處理與運算高維度的向量，從而實現語意理解與生成。

- 思考 [第伍章 ☸](05----ai_orientations.zh-hant) AI 5 大導向（AI Orientations）的各種系統／設計思維視角，是如何運用`大語言模型網組合`，去構成有用的**知識組織方式**與**問題解決策略**。


