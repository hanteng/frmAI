---
tags:
  - 基礎模型
  - 生成模型
  - RLHF
---
# 😵‍💫🧞‍♀️大語言模型 {#sec-large-language-models}

`大語言模型`（Large Language Models, LLMs）因為 2022 年底代表性案例 [LLM聊天機器人](04-02-llm_chatbots.zh-hant) **ChatGPT**的迅速普及，不僅重燃了人們對人工智慧（AI）的熱情，甚至引發了對通用人工智慧（[AGI](02-04-agi.zh-hant)）的遐想。

為了簡要說明 `大語言模型`的關鍵組成技術，本條目依循介紹以下：

- 🧠 神經－符號合流的現代案例：從**注意力機制**到**轉換器架構**的成功，標誌**深度學習**重大突破。
- 🔮 自我監督學習的現代數字文本模型：透過**預訓練**機制，從**海量未標記資料**學習語言結構、知識與世界觀。
- 🔼 神經元思考：神經元思考：連結主義的深度學習透過 **監督式微調** 與行為主義的**強化學習**結合RLHF，將模型的能力與人類**偏好、價值觀**對齊。

為了闡明`大語言模型`，已有幾類 ❝腦補❞ 心智模型假說，解釋其功能及運作：

- 🎞🤯 **巨型自動完成機**（Giant Autocomplete Machine）：LLM 好比文本預測「極致自動完成」仙，能超越自動完成系統猜中下一個詞。
- 🗺️🧭 **巨型統計地圖**（Giant Statistical Map）：LLM 如同詞彙「路徑生成」神，能在高維語意地圖按最可能路徑前行，完成神回應。
- 🗜️😵‍💫 **網際網路文本有損壓縮**（Lossy Compression of Internet Text）：LLM 如「整個網路的海量文本」模型參數壓縮檔，因捨棄細節有損所以難免「產生幻覺」。
- 🎭🧞‍♀️ **人機腦補語言賽局**（Mutual Mental Fill-in Language Game）：LLM 好比聊天「對話藝術」精，能流暢多輪對話贏得使用者信任（❝腦補❞雙贏賽局），填補對話甚至心靈空缺。

## 🎞巨型自動完成機🤯

🏷️ **解釋**：LLM 本質上是一個極度先進的 `巨型自動完成機`（Giant Autocomplete Machine），為了要**預測**最有可能出現的下一個詞，LLM 基於海量文本資料中習得語言的統計模式，並據此[生成](06-05-analysis_generative.zh-hant.md) 連貫文本，參與流暢對話。這比喻說法由**Grant Sanderson** 等人推廣，突顯使用者感知到的是 LLM [模仿人類溝通](01-01-Turing_Test.zh-hant.md)的「智慧」[@Sanderson2023-gpt-visual-intro;@Manning2022-understanding-reasoning-with-llms]。

- 🎯 **解釋較準部份**：
    - 🤯 **簡單易懂**：用過當代搜索界面有「自動完成」或「自動補完」的使用者，能將LLM簡單理解為更強大的「接話器」。
    - 🎞 **核心機制**：精準捕捉模型根本的 [機率性關聯](04-01-probabilistic_association.zh-hant) 本質，與依序生成詞彙的過程。
    - ⚠️ **凸顯限制**：突顯其「[🌀統計流](04----statistical_ai.zh-hant.md)」機器學習預測本質，而非內建「[🏛️符號流](03----symbolic_ai.zh-hant.md)」AI 知識庫，直觀地解釋為何模型生成可能產生「幻覺」，而非查證事實。
- ❌ **解釋缺失部份**：
    - 🤔 **湧現能力**：難以解釋模型如何展現超越簡單預測的複雜推理、摘要等能力。誤以為僅是「表層模仿」，忽略其**深層語意建模**能力。
    - 🪞 **內部表徵**：未觸及模型形成複雜語言與概念內部表徵的「如何」運作。

此說法突顯 LLM 的 **[生成邏輯](06-05-analysis_generative.zh-hant.md)** 與 **[機器學習](04-05-machine_learning_models.zh-hant.md)** 依靠海量文本的面向，但未能充分說明**語境理解**、**推理**與**創造性輸出**的潛力機制。

***
## 🗺️巨型統計地圖🧭

🏷️ **解釋**：LLM 也可以被視為一張「巨型統計地圖」（Giant Statistical Map）。在這張多維度的語意地圖上，每個詞彙、概念與語境都被映射為一個[向量空間](04-07-vector_space.zh-hant.md)中的座標點。當輸入提示時，模型會根據統計學上最可能、最連貫的路徑，在這張地圖上「導航」，沿途生成詞彙，最終形成回應。這個比喻由 **Christopher Manning** 等學者提出，突顯 LLM 的核心在於 **語意結構** 的 **捕捉與導航** [@Manning2022-understanding-reasoning-with-llms]。

- 🎯 **解釋較準部分**：
    - 🌌📍 **向量嵌入**：正確揭示了 LLM 的基礎——**向量嵌入**（vector embeddings），以及如何透過語意相似性將詞彙與概念組織在高維空間中。
    - 🧭🔗 **關聯性推理**：有助於解釋模型如何理解概念之間的關係，而不僅僅是單詞的逐一拼接。
    - 🗺️📐 **結構化知識**：提供了一種直觀方式，將知識視覺化為「地圖」上的結構化表示，幫助理解模型如何在語境中進行推理。
- ❌ **解釋缺失部分**：
    - ❓😵‍💫 **直觀性不足**：對非技術背景的讀者而言，這個比喻較抽象，不如「自動完成」那樣容易理解。
    - 🎲🚶 **逐步生成**：雖然解釋了「路徑導航」，但未能清楚展示 LLM 如何透過機率分布逐步生成詞元，以及為何會選擇特定路徑。

此說法突顯 LLM 的 **知識表徵與語意導航能力**，將其比擬為一個能理解語意關係、進行抽象推理的「語言地圖系統」。然而，它忽略了 LLM 在生成過程中的**序列性與隨機性**，因此需要與其他比喻（如「自動完成機」）結合，才能更完整地呈現其運作原理。


## 🔄歷史演進🗿

`大語言模型` 發展史，是**深度學習**不斷朝向**語言理解**與**生成極限**邁進的濃縮歷程，歷經幾次關鍵的**技術革命**和**規模突破**，最終從經典自然語言處理（NLP）演進為當代的 **[生成式 AI](06-05-analysis_generative.zh-hant.md)**。

- 📜 **基礎奠定期（2013–2017）** ➠ **詞向量**（Word Embeddings，如 Word2Vec、GloVe）的出現，標誌語言模型從純的符號表示進入 **[向量空間](04-07-vector_space.zh-hant.md)**。隨後的 **RNN**（循環神經網路）和 **LSTM** 等序列模型成為主流，讓模型開始處理脈絡上下文依賴性，為語言理解奠定了 **[神經網路](04-03-neural_networks.zh-hant.md)** 的基礎。
- 🌌 **注意力機制與轉換器革命（2017）** ➠ **轉換器（Transformer）** 架構的發表 [@Vaswani2017-attention]，徹底改變了序列處理方式。它引入了**自注意力機制**（Self-Attention），使得模型能**同時**捕捉長距離依賴關係，克服了 RNN 的效率瓶頸，成為所有現代 LLM 的核心基石。
- 🔮 **預訓練模式確立（2018–2020）** ➠ **BERT**、**GPT-2** 等模型的誕生，確立了 「**預訓練 + 微調**」範式。模型透過在海量**未標記文本**上進行**自我監督學習**（Self-Supervised Learning），從而學習到通用的語言知識，這極大地提升了模型處理下游 NLP 任務的表現。
- 🔼 **規模與突現能力（2020–2022）** ➠ 隨著模型規模（參數數量）不斷擴大，達到數百億甚至數千億級別（如 GPT-3、PaLM），模型開始展現出「**突現能力**」（Emergent Abilities）[@Wei2022-emergent]。**思維鏈**（Chain-of-Thought, CoT）的關鍵技術成果，即透過引導模型輸出中間推理步驟，極大地提升了 LLM 在複雜數學、邏輯推理上的表現。這些能力標誌著 LLM 從語言工具轉變為**通用認知輔助工具**。
- 🤝 **對齊與人性化（2022–至今）** ➠ **人類回饋強化學習**（RLHF） 成為主流對齊技術。透過這個階段，模型（如 ChatGPT、GPT-4）的輸出被引導至更符合人類**偏好、價值觀和安全性**，從而在人機互動和部署應用方面取得巨大成功，使其能更廣泛地應用於實際生活與商業場景。
- 🌐 **多模態與具身智慧（2023–至今）** ➠ `大語言模型` 開始發展出**多模態 LLM**，與圖像、聲音等其他模態數據結合（如 Gemini、GPT-4V）[@OpenAI2023-gpt4]。同時，結合 **[具身智慧](08----embodied_ai.zh-hant.md)**（Embodied AI）的研究，正在探索讓 LLM 不僅能理解語言，還能與物理世界進行「**行動**」互動，向更具通用性的 AI 發展 [@Driess2023-PaLME]。

由此可見，`大語言模型` 的歷史演進是 **深度學習** 技術、**計算規模** 和 **數據可用性** 共同作用的結果，其核心突破是 **轉換器架構** 和 **自我監督學習**，為當代 AI 系統提供了語言理解與推理基礎，並產出**多模態 LLM** 及 具身智慧 等相關創新。

## 👉接下來🪸

- ⮦🚦 探究 [第陸篇 ❖](06----analytics_decisions.zh-hant.md)　**分析與決策 6 點**，探索 `LLM` [生成式 AI](06-05-analysis_generative.zh-hant.md) 對分析與決策的影響。
- ⮦🚥 探究 [第拾篇 🌉](10----ai_engineering.zh-hant.md)　**AI工程**，探索 `LLM` 的相關實踐及應用：
	* **10.1** 🌉🔗🌐 [API與MCP](10-01-API_MCP.zh-hant.md)（API/MCP）
	* **10.2** 🌉🤖🚨 [智能體可靠性與評估](10-02-agent_reliability_evaluation.zh-hant.md)（Agent Reliability & Evaluation）
	* **10.3** 🌉❔📌 [提示工程](10-03-prompt_engineering.zh-hant.md)（Prompt Engineering）
	* **10.4** 🌉🔗📒 [知識驅動生成（RAG）](10-04-retrieval_augmented_generation.zh-hant.md)（Retrieval-Augmented Generation）
	* **10.5** 🌉🪟🧭 [脈絡工程](10-05-context_engineering.zh-hant.md)（Context Engineering） 
	* **10.6** 🎁🌱🚀 [AI 產品經理](10-06-AI_PM.zh-hant.md)（AI Product Management）
- ⮦🚦 探究 [第肆篇 🌀](04----statistical_ai.zh-hant)　**統計流 AI**的其它條目，評估自己可不可以說明 `LLM` 和它們的關係，如下所述：
	- **🌀🎲🌿 [機率性關聯](04-01-probabilistic_association.zh-hant)**：`LLM`型透過**序列預測的訓練**，學習輸入文本與輸出詞元之間的**機率性關聯**，是其生成連貫文本的根本。
	- **🌀🧞‍♀️🗪 [LLM聊天機器人](04-02-llm_chatbots.zh-hant)**：本身就是一種複雜的`LLM`應用，利用龐大的參數和海量數據進行訓練，以實現自然語言的**理解與生成**。   
	- **🌀🪢🧠 [神經網路](04-03-neural_networks.zh-hant)**：現代 `LLM` 最為強大和流行的一類**神經網路**骨幹，是**轉換器架構**（Transformer），尤其是在處理**長距離依賴**和**大規模預訓練**方面。     
	- **🌀🛠️🤏 [特徵工程](04-04-feature_engineering.zh-hant)**：現代 `LLM` 的**注意力機制**是允許模型在處理序列時**加權關注**不同部分的輸入的特徵工程，是實現精確**語境理解**的關鍵。  
    - **🌀🌐🔗 [大語言模型網組合](04-06-llm_webassembly.zh-hant)**：**大語言模型網組合**的實現，依賴於高效能的 `LLM` 在網際網絡瀏覽器環境中完成如**推理引擎**等任務。 
    - **🌀🌌▦ [向量空間](04-07-vector_space.zh-hant)**：`LLM`在**向量空間**中進行操作，將詞彙和概念轉換為**向量表示**（Embeddings），並在此空間中尋找**語義模式**和進行**類比推理**。

